{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb80fd34-69fb-49df-a8c4-afd2b096ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "ours2orig = [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 254, 41, 42, 43, 44, 45,\n",
    "46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,\n",
    "65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85,\n",
    "86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,\n",
    "104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
    "119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 136, 138, 132,\n",
    "133, 134, 135, 220, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
    "153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
    "168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 185, 186,\n",
    "187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
    "202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216,\n",
    "217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
    "233, 137, 234, 235, 236, 237, 238, 239, 240, 241, 250, 251, 255, 256, 257,\n",
    "258, 259, 260, 261, 262, 263, 242, 243, 244, 245, 0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
    "9, 10, 11, 83, 84, 131, 139, 140, 141, 181, 182, 183, 184, 246, 247, 248,\n",
    "249, 252, 253]\n",
    "\n",
    "def vec2mat(v):\n",
    "    a,b = np.triu_indices(264,1)\n",
    "    m = np.zeros((264,264))\n",
    "    m[a,b] = v\n",
    "    return m+m.T\n",
    "\n",
    "def remap(fc, roimap=ours2orig):\n",
    "    fc = fc[roimap,:]\n",
    "    fc = fc[:,roimap]\n",
    "    return fc\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf48325-5662-466e-be78-282bf533de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1155, 34716), (1155, 34716), (1155, 34716), (1155,), (1155,), (1155,)]\n"
     ]
    }
   ],
   "source": [
    "# Load FC\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "pncdir = '/home/anton/Documents/Tulane/Research/data-sav/anton/cohorts/PNC/'\n",
    "lowrankdir = '/home/anton/Documents/Tulane/Research/Work/ContrastiveLearning/PNC/Top10/'\n",
    "pncdemo = pickle.load(open(f'{pncdir}/demographics.pkl', 'rb'))\n",
    "no_snps_subs = pickle.load(open('/home/anton/Documents/Tulane/Research/ImageNomer/data/PNC/fc_subs_no_snps.pkl', 'rb'))\n",
    "\n",
    "rest = []\n",
    "nback = []\n",
    "emoid = []\n",
    "restmat = []\n",
    "nbackmat = []\n",
    "emoidmat = []\n",
    "race = []\n",
    "sex = []\n",
    "age = []\n",
    "subids = []\n",
    "\n",
    "a,b = np.triu_indices(264,1)\n",
    "\n",
    "for sub in pncdemo['age_at_cnb']:\n",
    "    if sub in no_snps_subs:\n",
    "        continue\n",
    "    try:\n",
    "        ra = pncdemo['Race'][sub]\n",
    "        ag = pncdemo['age_at_cnb'][sub]\n",
    "        se = pncdemo['Sex'][sub]\n",
    "        if ra not in ['AA', 'EA']:\n",
    "            continue\n",
    "        ra = ra == 'AA'\n",
    "        se = se == 'M'\n",
    "        r = np.load(f'{pncdir}/fc/{sub}_task-rest_fc.npy')\n",
    "        n = np.load(f'{pncdir}/fc/{sub}_task-nback_fc.npy')\n",
    "        e = np.load(f'{pncdir}/fc/{sub}_task-emoid_fc.npy')\n",
    "        r = remap(vec2mat(r))\n",
    "        n = remap(vec2mat(n))\n",
    "        e = remap(vec2mat(e))\n",
    "        race.append(ra)\n",
    "        sex.append(se)\n",
    "        age.append(ag)\n",
    "        rest.append(r[a,b])\n",
    "        nback.append(n[a,b])\n",
    "        emoid.append(e[a,b])\n",
    "        subids.append(sub)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "rest = np.stack(rest)\n",
    "nback = np.stack(nback)\n",
    "emoid = np.stack(emoid)\n",
    "race = np.array(race).astype('int')\n",
    "sex = np.array(sex).astype('int')\n",
    "age = np.array(age)\n",
    "\n",
    "print([a.shape for a in [rest, nback, emoid, race, sex, age]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55384c16-f6d1-405e-89f0-cfd3ce47e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1155, 35621)\n"
     ]
    }
   ],
   "source": [
    "# Load big SNPs\n",
    "\n",
    "snps = []\n",
    "no_snps_subs = []\n",
    "\n",
    "snps_file = pickle.load(open('/home/anton/Documents/Tulane/Research/ImageNomer/data/PNC/snps_all_subs_big.pkl', 'rb'))\n",
    "\n",
    "for sub in subids:\n",
    "    if sub not in snps_file:\n",
    "        print(sub)\n",
    "        # no_snps_subs.append(sub)\n",
    "    snps.append(snps_file[sub])\n",
    "\n",
    "snps = np.stack(snps)\n",
    "snps[np.isnan(snps)] = 0\n",
    "\n",
    "print(snps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecee3f79-ff6d-40a9-b186-0fc6fb88aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00030872895499649397 -2.4671035241755628e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = np.concatenate([snps == 0, snps == 1, snps == 2], axis=1)\n",
    "reg_sex = LogisticRegression(C=100).fit(x, sex)\n",
    "sex_w = torch.from_numpy(reg_sex.coef_[0]).float().cuda()\n",
    "sex_i = reg_sex.intercept_[0]\n",
    "\n",
    "reg_race = LogisticRegression(C=100).fit(x, race)\n",
    "race_w = torch.from_numpy(reg_race.coef_[0]).float().cuda()\n",
    "race_i = reg_race.intercept_[0]\n",
    "\n",
    "print(sex_i, race_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f29c42-157b-456c-a78b-865416fd9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1958.0067, -1984.7115, -1968.2627, -1962.3083,        nan,        nan,\n",
      "        -1926.1584,        nan, -1991.4304,        nan,        nan, -2000.2439,\n",
      "        -1949.2017, -1985.9413, -1973.5046, -1974.4150, -1974.7781,        nan,\n",
      "        -1992.0706, -2023.5323,        nan, -1999.6013,        nan, -2000.4510,\n",
      "               nan, -1955.8319,        nan, -1981.3052, -1940.6836,        nan,\n",
      "        -1985.1854, -1981.6063, -1973.7458,        nan,        nan, -1956.8263,\n",
      "               nan, -1965.9131,        nan, -1990.7683, -2001.2771,        nan,\n",
      "               nan,        nan, -1979.7769, -2000.0646, -1981.8816, -1971.1108,\n",
      "               nan,        nan,        nan,        nan, -1973.8137,        nan,\n",
      "        -1967.8628,        nan, -1981.9492, -1966.6049, -1982.8804, -1975.8392,\n",
      "               nan,        nan, -1995.8494, -2002.4097, -1977.5991,        nan,\n",
      "        -1978.1053, -1976.7570,        nan,        nan,        nan,        nan,\n",
      "               nan, -1965.1519,        nan, -2020.9875,        nan,        nan,\n",
      "        -1982.9424,        nan, -1956.8621, -1981.3870, -1965.9235,        nan,\n",
      "        -1988.9417,        nan,        nan, -1992.1045, -1992.9404, -1975.3003,\n",
      "               nan, -1980.3596,        nan,        nan,        nan,        nan,\n",
      "        -1956.5803,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1989.4006, -1987.4258,        nan,\n",
      "        -1996.8800,        nan,        nan, -1994.3789, -1980.5062, -1972.6824,\n",
      "        -1993.5276, -1983.3951,        nan,        nan, -1934.2906,        nan,\n",
      "               nan,        nan,        nan,        nan, -1977.8335, -1962.1991,\n",
      "               nan, -1975.4130,        nan,        nan,        nan, -1978.0167,\n",
      "               nan, -1992.9144, -1962.2400,        nan, -1969.6984, -2040.8110,\n",
      "        -1968.7590,        nan,        nan,        nan, -1980.3871,        nan,\n",
      "               nan, -1972.8463,        nan,        nan,        nan, -1976.1039,\n",
      "               nan,        nan, -1973.4680,        nan,        nan, -1960.8381,\n",
      "               nan,        nan, -1979.2737,        nan,        nan,        nan,\n",
      "               nan,        nan, -1983.2915,        nan,        nan, -1987.2081,\n",
      "        -1967.3691, -1959.1624, -1951.6921, -1961.5111,        nan,        nan,\n",
      "        -1946.5095, -1970.3501, -1963.5618,        nan, -1957.4905,        nan,\n",
      "               nan,        nan,        nan, -1944.8560, -1981.5494, -1981.3514,\n",
      "               nan, -1943.7444, -1992.0995, -2006.9324, -2032.8712,        nan,\n",
      "        -1995.5590,        nan, -1992.8235,        nan,        nan,        nan,\n",
      "               nan, -1983.2471, -1972.5757,        nan,        nan, -2024.0616,\n",
      "               nan, -1985.8092,        nan, -1974.0239, -1996.8772, -1967.2788,\n",
      "               nan, -2011.7990,        nan, -1939.4791,        nan, -1950.5975,\n",
      "               nan,        nan,        nan, -2027.6299, -1986.7562, -1988.5498,\n",
      "        -1974.2452, -1965.3328, -1966.8141,        nan,        nan, -1983.9235,\n",
      "        -1997.7765,        nan, -1937.6836, -2002.4094,        nan,        nan,\n",
      "               nan,        nan, -1965.8223, -1976.7786, -1962.1243,        nan,\n",
      "        -1970.3230,        nan, -1994.6370,        nan, -2009.8418, -1989.1157,\n",
      "        -1963.8672,        nan, -1981.9486, -1944.8076,        nan,        nan,\n",
      "               nan,        nan,        nan, -1977.6013,        nan,        nan,\n",
      "               nan, -1978.0132, -1982.3672,        nan,        nan, -2006.1367,\n",
      "               nan,        nan,        nan, -1963.3884, -1939.8484, -1978.6311,\n",
      "               nan,        nan,        nan, -1995.8975, -1946.2068,        nan,\n",
      "        -1959.8159,        nan, -1958.8871,        nan, -2012.3877,        nan,\n",
      "        -1983.4102,        nan,        nan,        nan, -1957.3074, -1975.0735,\n",
      "               nan, -1975.3500,        nan, -1957.2131, -1936.7747, -1951.2415,\n",
      "               nan, -1998.4606, -1984.1598, -1983.1423, -1964.8573,        nan,\n",
      "        -1989.9205,        nan,        nan, -1999.8611,        nan,        nan,\n",
      "               nan, -1964.4662,        nan,        nan, -1997.8868,        nan,\n",
      "        -1978.3604, -1963.3367,        nan, -1956.3516,        nan,        nan,\n",
      "        -1965.7793, -1947.7312,        nan,        nan,        nan, -1994.3997,\n",
      "               nan, -1985.3278, -1961.7056, -1982.4390,        nan,        nan,\n",
      "        -1996.5642,        nan,        nan,        nan, -1958.1261, -1990.4583,\n",
      "               nan,        nan,        nan,        nan, -1942.2397, -1976.9098,\n",
      "        -1990.6379,        nan,        nan,        nan,        nan, -1995.0890,\n",
      "        -1965.9253, -1982.0593, -1991.1611, -1968.5452, -1968.8428,        nan,\n",
      "        -1945.8542, -1948.1079, -1943.7356, -1994.5581, -1971.7278,        nan,\n",
      "        -2010.3137,        nan,        nan, -1986.7574, -1983.4075, -1985.1625,\n",
      "               nan,        nan,        nan, -1959.6921,        nan,        nan,\n",
      "        -1977.8953, -1939.2535,        nan, -1985.4994,        nan, -1955.5073,\n",
      "        -2012.0564, -1945.0098, -2013.0369, -1959.3695, -2007.4651, -1997.3475,\n",
      "        -1965.8340, -1960.8699,        nan, -1968.3135,        nan, -2022.4674,\n",
      "               nan,        nan,        nan, -1958.6556, -1974.5140,        nan,\n",
      "        -2039.4615, -1948.3770,        nan,        nan,        nan,        nan,\n",
      "               nan, -1997.2074, -1996.0928,        nan, -1989.1708,        nan,\n",
      "               nan, -1975.2457,        nan,        nan, -1995.3923, -1995.7218,\n",
      "        -1935.4429,        nan,        nan, -1978.0719,        nan, -1982.4430,\n",
      "               nan,        nan, -1971.6219, -1990.4508,        nan, -1994.3539,\n",
      "        -1979.0474, -1939.5232,        nan, -1998.1487, -1994.9086,        nan,\n",
      "               nan,        nan, -1972.0972, -1984.2355, -1979.4854, -1991.2610,\n",
      "               nan, -2031.6626, -1989.7102, -1987.4639,        nan,        nan,\n",
      "        -1964.3260, -1965.0337, -2009.3279, -1971.6851,        nan,        nan,\n",
      "               nan, -1961.5686,        nan,        nan, -1958.2522,        nan,\n",
      "        -1960.4395, -1970.4752, -1977.2487, -1977.6353,        nan, -1996.0118,\n",
      "               nan, -1954.5471,        nan, -1945.6365, -1973.2233,        nan,\n",
      "        -2017.2136,        nan,        nan,        nan, -1979.0168, -1979.8892,\n",
      "        -1958.6967,        nan, -1947.1626,        nan,        nan, -1930.9727,\n",
      "        -1982.2467, -1984.2920, -1979.0518, -1960.1538, -1995.1798, -1959.5778,\n",
      "        -2004.6384,        nan,        nan,        nan,        nan, -1945.0615,\n",
      "        -1976.4299,        nan,        nan, -2003.7552,        nan,        nan,\n",
      "               nan, -1999.4847,        nan, -1961.7180,        nan, -1981.6443,\n",
      "               nan,        nan,        nan, -1987.3003, -2007.9966, -1978.8879,\n",
      "               nan,        nan,        nan,        nan, -1973.8969, -1986.8076,\n",
      "        -1974.6334, -1983.9756,        nan, -1955.5957, -1963.8320,        nan,\n",
      "        -1987.1783,        nan,        nan, -1963.9211, -1986.7557, -1974.2010,\n",
      "               nan,        nan,        nan, -1994.7316, -1936.8745, -1986.7953,\n",
      "        -1995.8577,        nan,        nan,        nan, -1974.5823,        nan,\n",
      "        -2016.5098,        nan, -2002.3549,        nan, -1981.3247, -1971.9138,\n",
      "        -1983.4922, -2011.5950,        nan, -1988.6570,        nan,        nan,\n",
      "               nan, -1976.8545,        nan, -1969.3361, -1978.9878,        nan,\n",
      "        -1999.9707,        nan, -1979.3804, -1998.0308, -1963.9758,        nan,\n",
      "        -1997.1891,        nan, -1981.7537, -2031.2715, -1962.7449,        nan,\n",
      "               nan, -1972.1531,        nan, -1962.9613,        nan,        nan,\n",
      "        -1990.7145, -1994.4314, -1971.7070, -1974.2068,        nan, -2006.2738,\n",
      "        -2006.9102, -2000.6125, -1997.4812,        nan, -1963.0657,        nan,\n",
      "        -2002.8502,        nan, -1949.4456, -2003.9043,        nan, -1981.8761,\n",
      "               nan,        nan, -1966.9851,        nan, -1999.9779, -1979.4866,\n",
      "        -2011.9207, -2000.2649, -1963.5193, -1981.2798,        nan,        nan,\n",
      "               nan,        nan, -1972.2876, -1968.7303,        nan,        nan,\n",
      "        -1954.7351,        nan, -1983.9471, -1969.8530, -1983.7009,        nan,\n",
      "        -1953.1304, -2007.6875, -1977.5608, -1938.9304, -1976.0918,        nan,\n",
      "        -1972.0396,        nan,        nan, -2003.0249,        nan,        nan,\n",
      "        -1958.8386,        nan,        nan,        nan, -1963.9034, -1997.7412,\n",
      "        -1985.8124,        nan,        nan,        nan, -1957.9283,        nan,\n",
      "               nan,        nan,        nan, -1952.4749,        nan,        nan,\n",
      "               nan, -1971.4558,        nan, -1987.8735,        nan, -2000.0125,\n",
      "        -1983.8127,        nan, -1960.2122, -1975.8340,        nan,        nan,\n",
      "        -1982.8721,        nan, -2012.0938,        nan, -1983.8267, -1975.8652,\n",
      "        -1946.5876, -1977.5209, -1984.4404, -1977.0660, -1998.9561, -1949.2358,\n",
      "        -1958.1716, -1950.1504,        nan,        nan, -2016.3813, -1978.4659,\n",
      "               nan, -1978.7596,        nan,        nan,        nan,        nan,\n",
      "               nan, -1970.5994,        nan,        nan, -1989.2965,        nan,\n",
      "        -1971.5891,        nan,        nan, -1942.2357, -2006.5444, -2002.7732,\n",
      "        -1994.2843,        nan,        nan,        nan, -1967.4958,        nan,\n",
      "               nan,        nan,        nan, -1959.0165,        nan,        nan,\n",
      "               nan,        nan, -1959.7357,        nan,        nan, -1947.9902,\n",
      "        -1981.5170, -2027.0668, -1985.5967, -1936.3481,        nan,        nan,\n",
      "               nan,        nan,        nan, -2019.5310,        nan, -1991.8196,\n",
      "        -2005.8445,        nan,        nan, -2022.6593,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan, -2001.8569,\n",
      "               nan, -1963.3781,        nan,        nan,        nan,        nan,\n",
      "               nan, -1960.8862, -2003.6348,        nan, -1959.4177,        nan,\n",
      "               nan,        nan, -1987.2959, -2000.9270,        nan,        nan,\n",
      "               nan,        nan, -1971.0789, -1985.4465, -1986.5939,        nan,\n",
      "        -1995.8777,        nan, -1977.1682, -1983.6250, -1966.8324,        nan,\n",
      "        -1967.4325, -1979.0470,        nan, -1973.7271,        nan, -2012.0734,\n",
      "               nan, -2028.5210, -1988.5581, -1954.1937,        nan,        nan,\n",
      "               nan,        nan, -1979.2373,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1999.1313,        nan,        nan,\n",
      "               nan,        nan, -1968.9888,        nan, -1958.5173,        nan,\n",
      "        -1992.2554, -2004.4097], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "0 1.0993 nan 8.7182 0.8172 0.636\n",
      "tensor([       nan,        nan,        nan, -1041.7119,        nan,        nan,\n",
      "               nan, -1043.4167,        nan, -1043.4193, -1044.8264, -1053.8125,\n",
      "               nan, -1035.0914,        nan,        nan,        nan, -1042.3511,\n",
      "        -1049.6169, -1046.0028,        nan, -1051.4011, -1052.1699, -1042.5153,\n",
      "        -1060.2372, -1050.8423, -1051.9791,        nan, -1053.9467, -1043.1678,\n",
      "               nan,        nan,        nan,        nan, -1041.5220, -1047.2246,\n",
      "        -1056.7939, -1038.0460,        nan,        nan, -1045.7928, -1058.9778,\n",
      "        -1043.7072, -1036.3948, -1055.2712,        nan, -1060.0227,        nan,\n",
      "               nan, -1052.7327, -1043.9321,        nan,        nan, -1053.7190,\n",
      "        -1054.2961,        nan,        nan,        nan, -1042.4130,        nan,\n",
      "               nan,        nan, -1057.6929, -1046.9482, -1043.1694, -1063.4941,\n",
      "        -1050.6785, -1050.4781,        nan,        nan, -1053.1843, -1053.0626,\n",
      "        -1050.0630,        nan,        nan, -1043.8757, -1050.3135,        nan,\n",
      "               nan,        nan, -1041.0391, -1050.2437, -1041.6906,        nan,\n",
      "        -1058.9375,        nan, -1039.7877,        nan,        nan,        nan,\n",
      "        -1060.6963,        nan, -1041.1409,        nan, -1049.2698, -1044.9788,\n",
      "               nan, -1049.7039, -1054.3918,        nan,        nan, -1047.7886,\n",
      "               nan, -1054.0505, -1042.4858,        nan,        nan, -1052.4659,\n",
      "               nan,        nan,        nan, -1015.4170, -1040.6956,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1043.9958,        nan, -1051.2903, -1044.7363,        nan, -1041.7321,\n",
      "        -1055.2549, -1049.1646,        nan,        nan, -1041.6755, -1056.1079,\n",
      "               nan,        nan,        nan, -1050.2021,        nan, -1057.9143,\n",
      "        -1052.7585, -1048.5938, -1051.4314,        nan, -1044.5598,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan, -1035.6385,\n",
      "        -1046.5964,        nan,        nan, -1053.7910, -1052.0975,        nan,\n",
      "        -1051.0972, -1045.2112, -1040.7278, -1039.8687,        nan, -1047.9111,\n",
      "        -1042.9856, -1054.3796, -1040.7916, -1055.6733, -1037.4482,        nan,\n",
      "        -1048.7502,        nan, -1047.7017,        nan, -1038.0923,        nan,\n",
      "        -1054.9988, -1039.2269, -1046.1063,        nan, -1051.6143,        nan,\n",
      "               nan, -1043.6267,        nan, -1039.0981,        nan, -1046.0522,\n",
      "        -1039.9020,        nan,        nan, -1046.9768, -1047.1376,        nan,\n",
      "        -1046.7819, -1043.8599, -1058.7427,        nan, -1036.4305, -1044.1316,\n",
      "        -1049.9944, -1041.6187,        nan,        nan,        nan, -1048.7283,\n",
      "               nan,        nan,        nan, -1044.4772,        nan, -1053.5836,\n",
      "               nan,        nan,        nan,        nan, -1045.2139,        nan,\n",
      "               nan,        nan, -1040.7892,        nan,        nan,        nan,\n",
      "        -1048.8253,        nan,        nan, -1046.8302, -1045.2994, -1039.4962,\n",
      "        -1039.8455, -1047.6841, -1043.6396, -1052.1951,        nan,        nan,\n",
      "        -1046.3398,        nan, -1052.7341, -1055.9688, -1058.6049, -1044.5857,\n",
      "               nan, -1047.4426,        nan,        nan, -1042.2327, -1048.7140,\n",
      "               nan, -1048.6486, -1043.0549, -1052.1268,        nan,        nan,\n",
      "        -1037.8479, -1042.4688,        nan, -1051.7365, -1060.4875, -1049.3982,\n",
      "               nan, -1038.9194,        nan, -1036.5408,        nan,        nan,\n",
      "               nan,        nan, -1041.6130,        nan,        nan, -1051.4739,\n",
      "        -1051.0874, -1037.1057, -1039.3477,        nan, -1046.0945,        nan,\n",
      "               nan, -1045.7573,        nan,        nan,        nan, -1039.2598,\n",
      "               nan, -1051.4290,        nan,        nan,        nan,        nan,\n",
      "        -1046.9542,        nan,        nan,        nan, -1060.7625,        nan,\n",
      "        -1046.8313, -1051.4102, -1054.2537,        nan, -1038.4890,        nan,\n",
      "        -1047.9177, -1050.4674, -1041.9572, -1042.7227, -1040.4972, -1050.5840,\n",
      "        -1039.2168,        nan, -1043.8687,        nan, -1043.6130, -1039.0952,\n",
      "        -1019.5757,        nan, -1039.5864, -1053.6577, -1044.0002,        nan,\n",
      "               nan,        nan,        nan, -1045.5243, -1042.7070,        nan,\n",
      "        -1052.4197, -1036.3699, -1042.4043, -1047.4957, -1039.4624,        nan,\n",
      "               nan,        nan,        nan, -1050.9694,        nan, -1049.9668,\n",
      "        -1055.3372,        nan,        nan,        nan, -1056.3379,        nan,\n",
      "        -1044.7314,        nan, -1041.0449,        nan,        nan, -1042.2897,\n",
      "        -1041.0598, -1049.9354,        nan,        nan, -1045.7615,        nan,\n",
      "               nan,        nan, -1040.2136, -1045.8369,        nan, -1045.3741,\n",
      "        -1037.5410, -1042.6836, -1047.4297,        nan,        nan,        nan,\n",
      "               nan,        nan, -1052.8923,        nan, -1047.9675,        nan,\n",
      "        -1052.8005,        nan,        nan,        nan, -1044.6642, -1035.4585,\n",
      "        -1043.3838,        nan,        nan, -1042.1340,        nan, -1037.2543,\n",
      "               nan,        nan, -1048.4060,        nan, -1039.9647, -1059.3955,\n",
      "        -1048.9216,        nan,        nan, -1038.6938,        nan,        nan,\n",
      "               nan, -1042.4927, -1045.8473, -1048.6206,        nan, -1056.0732,\n",
      "        -1054.7727, -1046.9880,        nan, -1050.1914,        nan, -1053.2927,\n",
      "               nan,        nan, -1049.3979,        nan, -1041.9832, -1050.0046,\n",
      "        -1048.9302, -1039.0925, -1045.3820, -1057.6111,        nan, -1040.1282,\n",
      "               nan,        nan,        nan,        nan, -1050.0732, -1038.4189,\n",
      "               nan,        nan, -1036.3047,        nan,        nan, -1042.2844,\n",
      "               nan,        nan, -1040.5580, -1045.7371, -1045.6790,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1044.3013, -1056.4873, -1043.1383,        nan,        nan,        nan,\n",
      "               nan, -1037.1870, -1051.8594,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1045.7297,        nan, -1054.7213, -1050.1533,        nan, -1047.4219,\n",
      "               nan,        nan,        nan, -1035.9838, -1044.6838, -1050.3291,\n",
      "               nan, -1039.4686,        nan, -1048.1331,        nan, -1046.2605,\n",
      "        -1044.5526, -1039.2742, -1050.1643, -1042.2285,        nan, -1034.1156,\n",
      "               nan,        nan, -1042.0709,        nan,        nan,        nan,\n",
      "        -1042.6127, -1043.6233,        nan,        nan, -1044.5419, -1047.3408,\n",
      "        -1043.0159, -1041.7822, -1041.1665, -1044.0265,        nan, -1050.1719,\n",
      "        -1050.0759, -1045.9774, -1041.9169,        nan, -1032.5840,        nan,\n",
      "        -1043.5249,        nan,        nan, -1040.3997, -1049.9623,        nan,\n",
      "        -1048.0533,        nan,        nan, -1049.7748, -1050.9962, -1043.3518,\n",
      "               nan, -1040.2439, -1052.6873, -1043.4948, -1043.1283,        nan,\n",
      "               nan,        nan, -1055.7401,        nan,        nan, -1046.9663,\n",
      "        -1042.4734, -1046.2122, -1048.0681,        nan,        nan, -1044.1130,\n",
      "        -1054.0374, -1042.4385,        nan, -1039.5620, -1042.5376, -1053.5144,\n",
      "        -1048.4854,        nan, -1040.3992,        nan,        nan,        nan,\n",
      "        -1050.8621, -1041.8538, -1052.9058, -1046.9152, -1051.8323,        nan,\n",
      "               nan,        nan, -1052.5997,        nan,        nan, -1043.7219,\n",
      "        -1044.7874, -1050.7849,        nan,        nan,        nan, -1049.5842,\n",
      "        -1039.9500,        nan, -1044.5999, -1041.4597, -1050.6660, -1056.2468,\n",
      "               nan,        nan,        nan, -1036.4694,        nan,        nan,\n",
      "               nan,        nan, -1042.4685, -1047.7429,        nan,        nan,\n",
      "               nan,        nan, -1042.8680,        nan,        nan, -1048.9614,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan, -1048.4060, -1045.1399, -1043.1708,        nan,\n",
      "               nan,        nan, -1044.7336,        nan, -1039.3557, -1039.6495,\n",
      "               nan, -1038.9905,        nan, -1042.2119,        nan, -1051.9216,\n",
      "        -1057.6863, -1040.3940,        nan, -1044.3184, -1042.2103, -1047.8474,\n",
      "               nan, -1047.2682,        nan, -1057.9578,        nan,        nan,\n",
      "               nan,        nan,        nan, -1051.3949, -1046.9539,        nan,\n",
      "        -1046.7374,        nan,        nan, -1046.5464, -1040.5128,        nan,\n",
      "        -1040.2894, -1053.6886,        nan,        nan, -1044.5162, -1042.1742,\n",
      "        -1047.0066, -1046.9526,        nan, -1045.8386,        nan,        nan,\n",
      "               nan,        nan,        nan, -1046.1134,        nan,        nan,\n",
      "               nan,        nan, -1045.9890, -1045.1216, -1046.5587, -1045.5906,\n",
      "        -1041.4576, -1038.6438,        nan, -1039.5896, -1048.9634,        nan,\n",
      "        -1048.7743, -1041.2263, -1051.7311,        nan,        nan,        nan,\n",
      "        -1042.2644,        nan, -1047.4812,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1049.8433, -1043.1851,        nan,\n",
      "        -1045.1478, -1054.2292, -1044.0358, -1052.0653, -1048.4534,        nan,\n",
      "               nan, -1044.1189,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1039.6580, -1048.2000,\n",
      "        -1051.6147, -1044.0835,        nan,        nan,        nan, -1046.4907,\n",
      "        -1050.3259,        nan,        nan,        nan,        nan, -1039.9558,\n",
      "        -1048.3414, -1043.7698, -1045.3577,        nan, -1040.7550, -1056.4578,\n",
      "        -1038.4502,        nan, -1051.5940,        nan,        nan, -1053.4789,\n",
      "        -1039.7411,        nan,        nan, -1045.4512, -1036.3396,        nan,\n",
      "               nan,        nan, -1039.8528, -1062.8416,        nan,        nan,\n",
      "        -1058.1042, -1053.9554, -1047.0804,        nan, -1047.2429, -1047.3070,\n",
      "               nan, -1038.9199,        nan, -1036.6395,        nan,        nan,\n",
      "        -1047.2596,        nan,        nan, -1046.7847,        nan, -1040.7369,\n",
      "        -1035.9475, -1051.9691,        nan,        nan, -1045.1427,        nan,\n",
      "               nan,        nan,        nan, -1043.7410, -1054.8811, -1048.3276,\n",
      "               nan,        nan, -1053.2209,        nan, -1040.6619,        nan,\n",
      "               nan,        nan, -1053.4519,        nan, -1043.7131, -1048.8303,\n",
      "               nan, -1037.8118, -1036.8108, -1051.1040,        nan, -1039.4453,\n",
      "               nan,        nan,        nan,        nan, -1051.7180,        nan,\n",
      "        -1047.8887,        nan], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "tensor([-1084.5078,        nan, -1098.2720, -1095.1467, -1075.7571, -1085.3263,\n",
      "        -1075.2312, -1085.9355, -1089.1667,        nan, -1088.8650,        nan,\n",
      "               nan,        nan, -1073.3555,        nan, -1069.6047,        nan,\n",
      "               nan, -1085.1613, -1091.4319, -1073.0017,        nan, -1083.4265,\n",
      "        -1071.1260, -1076.2114,        nan, -1091.0762, -1077.7333, -1085.2272,\n",
      "        -1091.8290,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1077.7378, -1084.8295, -1086.1617, -1088.0098, -1068.8379, -1079.6311,\n",
      "        -1084.4836,        nan, -1078.5911, -1073.5112,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1052.5524,        nan,\n",
      "               nan,        nan,        nan, -1068.6423, -1095.1160, -1082.0864,\n",
      "               nan, -1085.4385, -1065.8726,        nan,        nan,        nan,\n",
      "               nan,        nan, -1075.4904, -1070.9448, -1071.5759,        nan,\n",
      "        -1076.4637, -1071.8815,        nan,        nan,        nan, -1087.4656,\n",
      "               nan,        nan, -1092.3218,        nan, -1092.3713, -1076.5635,\n",
      "        -1076.1442, -1074.7188, -1087.4209,        nan, -1098.3357, -1070.0635,\n",
      "        -1069.4670, -1099.8483,        nan, -1065.0105,        nan, -1089.6816,\n",
      "        -1084.6948, -1091.5710,        nan,        nan, -1073.8552, -1076.2170,\n",
      "        -1083.6920,        nan,        nan,        nan, -1084.4941,        nan,\n",
      "               nan, -1093.3004, -1072.0153, -1057.4625,        nan,        nan,\n",
      "        -1061.7756, -1071.6722,        nan,        nan,        nan, -1079.3700,\n",
      "               nan,        nan,        nan,        nan, -1094.1685,        nan,\n",
      "        -1078.5066, -1084.1917,        nan, -1082.1266,        nan, -1078.9158,\n",
      "        -1080.2037, -1088.0862,        nan, -1072.1101,        nan,        nan,\n",
      "               nan, -1069.4419, -1074.4021, -1069.3962,        nan, -1071.8599,\n",
      "        -1078.6752, -1080.5662,        nan, -1083.8778,        nan, -1091.4353,\n",
      "               nan,        nan,        nan, -1076.7468,        nan,        nan,\n",
      "               nan,        nan, -1085.4380,        nan,        nan, -1070.0476,\n",
      "        -1086.4431,        nan, -1088.0718, -1075.5742,        nan,        nan,\n",
      "        -1069.5841,        nan, -1086.4608,        nan,        nan, -1077.3375,\n",
      "        -1080.1384,        nan, -1084.6093, -1092.6963,        nan,        nan,\n",
      "               nan,        nan, -1086.8064, -1088.5546,        nan,        nan,\n",
      "        -1084.9768,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1077.9683, -1087.1613,        nan,        nan,        nan, -1085.0466,\n",
      "        -1080.7236,        nan, -1085.4312, -1078.2922, -1068.9591, -1065.6508,\n",
      "        -1085.7965,        nan, -1095.4749,        nan, -1066.9309, -1079.5935,\n",
      "        -1083.1604,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1057.9463,        nan, -1070.6045,\n",
      "               nan, -1085.9709, -1069.7969,        nan, -1082.8513, -1088.5751,\n",
      "               nan, -1072.5715, -1082.9303,        nan,        nan, -1072.8091,\n",
      "        -1086.8864, -1091.4072, -1071.2036, -1092.8688,        nan,        nan,\n",
      "        -1069.4214, -1086.8806,        nan,        nan,        nan, -1078.2849,\n",
      "        -1073.6650, -1073.1147, -1082.2789, -1069.0352,        nan,        nan,\n",
      "        -1087.4584, -1086.4612, -1074.4426,        nan, -1076.6025, -1073.2915,\n",
      "               nan, -1090.2678,        nan, -1091.9802, -1077.6909, -1088.5864,\n",
      "               nan,        nan,        nan,        nan, -1089.5519, -1069.6388,\n",
      "               nan, -1093.9319, -1083.1418, -1062.4709, -1084.2781, -1085.6587,\n",
      "        -1078.5175, -1076.6245,        nan,        nan, -1079.7910,        nan,\n",
      "               nan, -1067.6387,        nan, -1085.4734,        nan, -1087.3468,\n",
      "               nan,        nan,        nan, -1070.3428,        nan,        nan,\n",
      "        -1094.4521, -1071.9700,        nan, -1094.5535,        nan,        nan,\n",
      "        -1077.9214, -1072.6770, -1090.3999,        nan,        nan, -1069.5232,\n",
      "               nan, -1074.2747,        nan, -1069.5583,        nan, -1091.5876,\n",
      "        -1065.7965, -1078.5852,        nan, -1074.4042,        nan,        nan,\n",
      "        -1075.4764,        nan, -1071.0833, -1087.5979,        nan,        nan,\n",
      "        -1070.1931,        nan, -1084.9181, -1068.6482,        nan,        nan,\n",
      "        -1069.5298, -1079.2043, -1082.4792, -1075.5026, -1073.4847,        nan,\n",
      "        -1073.4827, -1088.2040,        nan,        nan, -1077.9089,        nan,\n",
      "        -1081.7361,        nan,        nan, -1094.5394, -1089.1221, -1084.4178,\n",
      "               nan,        nan,        nan,        nan, -1079.1865,        nan,\n",
      "        -1080.8379,        nan,        nan, -1093.7201,        nan, -1080.7340,\n",
      "               nan, -1090.0178, -1083.3434, -1092.5902, -1081.6752,        nan,\n",
      "               nan, -1067.1262, -1070.6997,        nan, -1069.2932,        nan,\n",
      "        -1088.6434,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan, -1082.1082, -1093.0187, -1070.4070, -1099.7158,        nan,\n",
      "               nan, -1067.8866,        nan, -1091.4006, -1087.6716,        nan,\n",
      "        -1076.7655,        nan, -1083.0530,        nan,        nan, -1082.5449,\n",
      "               nan,        nan, -1085.2461,        nan,        nan,        nan,\n",
      "        -1075.3938, -1072.3682, -1097.1194, -1075.6655,        nan,        nan,\n",
      "               nan,        nan, -1067.7930,        nan,        nan,        nan,\n",
      "               nan, -1091.3580,        nan,        nan, -1067.7437,        nan,\n",
      "               nan, -1069.0176, -1084.7161,        nan, -1066.7223, -1081.9272,\n",
      "               nan,        nan, -1090.0940, -1058.3743,        nan, -1063.5286,\n",
      "        -1075.7080, -1076.3794,        nan, -1084.9373,        nan, -1084.8867,\n",
      "               nan, -1087.5637, -1088.7820, -1081.0464, -1079.3488,        nan,\n",
      "        -1073.6567, -1065.1519,        nan, -1086.1925,        nan, -1076.2046,\n",
      "        -1069.1490,        nan,        nan,        nan,        nan, -1066.6614,\n",
      "               nan,        nan, -1084.2109, -1088.1863,        nan, -1085.2793,\n",
      "               nan, -1073.9106,        nan,        nan, -1091.8318,        nan,\n",
      "        -1088.2690,        nan, -1063.0303, -1097.4514, -1109.5068, -1056.9052,\n",
      "        -1081.5732, -1088.8252,        nan,        nan,        nan, -1100.4269,\n",
      "        -1083.0591, -1091.7987,        nan,        nan, -1086.6306, -1089.6030,\n",
      "        -1098.8167, -1093.6295,        nan,        nan, -1086.2866,        nan,\n",
      "        -1102.6422, -1087.0034, -1083.5555, -1094.0146, -1080.9368, -1067.5267,\n",
      "        -1092.2134,        nan, -1089.5259, -1062.4521, -1067.8181, -1072.4498,\n",
      "               nan,        nan,        nan, -1070.5894, -1089.6049,        nan,\n",
      "        -1084.1162, -1082.7122, -1086.3147,        nan, -1070.5371, -1081.0254,\n",
      "               nan,        nan,        nan, -1072.9902,        nan,        nan,\n",
      "        -1088.0034, -1091.8352,        nan, -1093.9556,        nan, -1068.7864,\n",
      "               nan, -1097.0342, -1074.1324,        nan, -1087.8396,        nan,\n",
      "        -1085.9519, -1069.3201, -1063.1023, -1088.8610,        nan,        nan,\n",
      "        -1045.9155, -1083.0692,        nan, -1083.1021,        nan, -1068.4977,\n",
      "               nan, -1088.2795,        nan,        nan,        nan,        nan,\n",
      "        -1065.7644,        nan,        nan, -1065.5542, -1076.8336, -1091.8716,\n",
      "        -1042.0825,        nan, -1072.0712,        nan, -1084.0447, -1087.0879,\n",
      "        -1084.7931, -1073.8654, -1081.2827,        nan, -1083.9182,        nan,\n",
      "               nan, -1104.7708,        nan,        nan, -1071.7852, -1076.8972,\n",
      "               nan, -1065.7896,        nan,        nan, -1081.5098,        nan,\n",
      "               nan, -1069.7916, -1090.0278,        nan,        nan,        nan,\n",
      "               nan, -1087.6350,        nan, -1091.0840, -1075.3459, -1065.0526,\n",
      "        -1087.2898,        nan, -1074.7805, -1084.5376, -1089.3591, -1071.1207,\n",
      "               nan,        nan,        nan,        nan, -1077.4814,        nan,\n",
      "               nan, -1091.3840,        nan,        nan, -1089.8606,        nan,\n",
      "        -1092.2646, -1089.9495,        nan, -1087.6423, -1081.3345, -1068.4757,\n",
      "        -1071.9302, -1089.8677,        nan,        nan, -1091.6492, -1072.3459,\n",
      "        -1075.3218, -1074.9751,        nan,        nan,        nan, -1079.9802,\n",
      "               nan, -1075.7322, -1067.5388, -1072.3429,        nan, -1088.2166,\n",
      "        -1090.5684, -1096.4852,        nan,        nan, -1086.1259, -1086.9678,\n",
      "        -1088.7720,        nan,        nan, -1077.2449, -1085.4358,        nan,\n",
      "               nan, -1088.0310, -1069.7776, -1066.9685,        nan, -1084.9421,\n",
      "               nan,        nan,        nan, -1072.1125,        nan, -1079.6831,\n",
      "        -1076.3021,        nan, -1094.9186,        nan, -1082.4680,        nan,\n",
      "        -1084.5911,        nan, -1079.4739, -1089.4377,        nan, -1089.6616,\n",
      "               nan, -1094.3235,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1081.0753, -1069.3247,        nan,\n",
      "               nan,        nan,        nan, -1069.1179, -1083.3556,        nan,\n",
      "        -1087.9094, -1073.4039,        nan, -1069.0361, -1087.6725, -1070.5754,\n",
      "               nan,        nan,        nan, -1077.7747, -1081.0565,        nan,\n",
      "               nan,        nan, -1070.8345, -1069.6748,        nan,        nan,\n",
      "        -1072.6056, -1082.1055, -1073.9568, -1092.7559, -1090.0432, -1085.0457,\n",
      "        -1080.4270,        nan, -1067.9303, -1090.3484,        nan,        nan,\n",
      "               nan,        nan, -1084.9756,        nan,        nan, -1072.1021,\n",
      "        -1087.0399, -1089.4556,        nan,        nan,        nan, -1074.8809,\n",
      "        -1093.2958, -1088.8168, -1082.4124,        nan, -1085.5059,        nan,\n",
      "               nan,        nan,        nan,        nan, -1091.1001, -1075.4944,\n",
      "               nan, -1067.9094, -1082.2759, -1074.1021, -1072.1460,        nan,\n",
      "               nan, -1079.7057,        nan, -1085.7930,        nan, -1073.1066,\n",
      "        -1085.1688,        nan, -1071.0833, -1100.8883,        nan,        nan,\n",
      "               nan, -1081.5177, -1079.8765,        nan,        nan,        nan,\n",
      "               nan, -1083.0131, -1070.9490, -1087.8923, -1075.2991,        nan,\n",
      "        -1089.7310, -1067.6167,        nan, -1065.3030,        nan, -1066.3805,\n",
      "        -1092.4172,        nan, -1072.9814, -1074.3608,        nan, -1074.6461,\n",
      "        -1070.5916, -1098.5632,        nan, -1072.9921, -1082.7732,        nan,\n",
      "        -1088.4868,        nan, -1099.0502, -1083.5518,        nan, -1093.7743,\n",
      "               nan,        nan], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "tensor([-1129.3794,        nan, -1122.8088, -1137.3679,        nan, -1116.2207,\n",
      "               nan, -1129.3549, -1114.4189,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1119.0503,        nan,\n",
      "        -1117.5293, -1121.5167,        nan, -1117.1101, -1104.9329,        nan,\n",
      "        -1122.0583, -1132.9771,        nan,        nan, -1111.7751, -1119.5155,\n",
      "        -1113.6787, -1118.0358, -1124.3734, -1138.3367, -1136.9038, -1112.2827,\n",
      "               nan, -1117.4155,        nan,        nan, -1109.5908,        nan,\n",
      "               nan, -1128.9026, -1121.6685, -1115.6196,        nan, -1114.8125,\n",
      "        -1123.7751, -1121.2922,        nan,        nan,        nan,        nan,\n",
      "        -1115.0663, -1109.2954,        nan, -1115.9948, -1111.8805, -1112.0128,\n",
      "               nan, -1125.6876,        nan, -1123.4407, -1110.5413,        nan,\n",
      "               nan,        nan,        nan, -1106.4471,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1112.5784, -1127.4066,\n",
      "               nan, -1131.5836,        nan, -1120.3228, -1129.4802,        nan,\n",
      "               nan, -1136.0662, -1122.7380,        nan, -1131.6360,        nan,\n",
      "        -1107.5422, -1112.0955,        nan, -1107.6631,        nan, -1121.9374,\n",
      "               nan,        nan,        nan, -1121.9236,        nan,        nan,\n",
      "               nan,        nan, -1123.6161, -1116.6006,        nan, -1108.8434,\n",
      "               nan,        nan, -1112.2521,        nan, -1131.9022, -1104.6379,\n",
      "        -1125.0476,        nan, -1115.9430,        nan,        nan, -1121.6370,\n",
      "        -1128.7671,        nan, -1109.8425,        nan, -1111.7189, -1139.0713,\n",
      "               nan,        nan, -1120.1177,        nan, -1130.8037,        nan,\n",
      "               nan,        nan, -1118.2959, -1122.6003,        nan,        nan,\n",
      "        -1127.8542, -1112.7334,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan, -1111.5569,        nan,        nan,        nan,\n",
      "        -1114.6704, -1132.8613,        nan,        nan,        nan, -1121.2764,\n",
      "               nan, -1123.3652,        nan,        nan,        nan,        nan,\n",
      "               nan, -1114.6099, -1126.7695,        nan, -1118.8690, -1127.9392,\n",
      "               nan,        nan, -1119.7395, -1123.1865,        nan, -1122.2480,\n",
      "               nan, -1123.5010,        nan, -1128.7043,        nan,        nan,\n",
      "        -1123.9308,        nan, -1113.2083,        nan,        nan, -1090.9692,\n",
      "               nan,        nan, -1121.2683, -1110.2273,        nan, -1125.5986,\n",
      "               nan, -1118.5063,        nan,        nan, -1117.6599,        nan,\n",
      "               nan, -1119.5627, -1128.8066,        nan,        nan, -1114.9946,\n",
      "        -1123.5454,        nan, -1117.5863,        nan,        nan, -1129.8298,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan, -1126.8528,        nan,        nan,        nan, -1112.9878,\n",
      "               nan,        nan,        nan,        nan, -1118.7899,        nan,\n",
      "               nan, -1124.5621,        nan,        nan,        nan,        nan,\n",
      "        -1114.4939, -1123.5410,        nan, -1116.5680,        nan, -1130.5166,\n",
      "        -1108.6979, -1113.6466, -1115.7888, -1128.4685, -1123.6489,        nan,\n",
      "        -1116.3986,        nan, -1133.9708, -1120.3408,        nan, -1118.9089,\n",
      "        -1119.4048, -1127.4724, -1109.6951,        nan,        nan,        nan,\n",
      "        -1118.3146,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan, -1117.2463,        nan, -1128.1504, -1115.4287, -1121.9891,\n",
      "               nan, -1115.0959, -1124.3518,        nan, -1124.2883,        nan,\n",
      "        -1121.7419,        nan, -1113.1090,        nan,        nan, -1125.0403,\n",
      "               nan, -1115.9572, -1118.0977,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1119.3477,        nan,\n",
      "               nan, -1108.3895, -1131.3856,        nan,        nan, -1132.7871,\n",
      "               nan,        nan, -1126.8206,        nan,        nan,        nan,\n",
      "        -1130.0315,        nan,        nan, -1125.4330, -1140.2795,        nan,\n",
      "               nan, -1122.2280, -1116.7490,        nan, -1124.0719, -1120.2986,\n",
      "               nan,        nan, -1108.9871,        nan,        nan,        nan,\n",
      "               nan, -1127.5071,        nan,        nan,        nan, -1125.7709,\n",
      "        -1128.3701, -1143.1074, -1126.2854,        nan,        nan, -1105.5906,\n",
      "               nan, -1121.6940, -1124.1464,        nan, -1118.6100,        nan,\n",
      "               nan, -1120.7290,        nan,        nan,        nan,        nan,\n",
      "               nan, -1104.7246,        nan,        nan, -1128.4651, -1129.1384,\n",
      "        -1114.4482, -1125.6814,        nan, -1108.5137, -1114.7147,        nan,\n",
      "               nan, -1126.2191,        nan, -1122.6514, -1135.8888,        nan,\n",
      "        -1111.4243,        nan, -1119.4674,        nan,        nan, -1115.4629,\n",
      "        -1128.0400, -1139.5437, -1109.7565, -1109.1970, -1122.0944, -1122.0442,\n",
      "        -1121.7738,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1111.1799, -1117.9282, -1130.8632,        nan, -1123.7504, -1117.5662,\n",
      "        -1125.0585,        nan, -1115.2560,        nan, -1123.1799,        nan,\n",
      "        -1151.6503, -1124.7292,        nan,        nan, -1115.7045, -1115.3613,\n",
      "        -1119.6345, -1107.2517, -1115.8041, -1127.1743,        nan,        nan,\n",
      "        -1128.0916, -1125.2234, -1117.2159, -1114.4985,        nan, -1113.0861,\n",
      "        -1131.1547,        nan, -1120.3525, -1118.0581,        nan, -1123.9480,\n",
      "               nan, -1113.1768,        nan,        nan,        nan, -1129.9070,\n",
      "        -1105.3525,        nan,        nan,        nan, -1119.4104,        nan,\n",
      "               nan,        nan, -1126.5289,        nan, -1117.9036,        nan,\n",
      "        -1124.7684, -1124.9434, -1118.5903, -1090.1713,        nan, -1123.4167,\n",
      "               nan, -1107.1206,        nan, -1128.6599,        nan, -1118.9338,\n",
      "               nan, -1142.8301, -1108.4979, -1130.8425,        nan,        nan,\n",
      "        -1111.0256,        nan, -1137.2554, -1127.4377,        nan,        nan,\n",
      "               nan, -1119.4124, -1120.4407, -1119.4077,        nan, -1123.0945,\n",
      "               nan,        nan,        nan, -1123.8247, -1100.6584,        nan,\n",
      "               nan, -1122.6227, -1113.9680,        nan,        nan,        nan,\n",
      "        -1129.3529,        nan,        nan,        nan,        nan, -1116.7676,\n",
      "        -1131.2798, -1118.0227, -1126.4908,        nan,        nan,        nan,\n",
      "        -1123.8009, -1115.7800, -1119.8505,        nan, -1121.8005,        nan,\n",
      "        -1124.7278,        nan, -1119.5271, -1066.1868,        nan, -1117.0287,\n",
      "        -1129.9495,        nan,        nan, -1113.1160, -1135.5991, -1127.7163,\n",
      "        -1127.7581,        nan,        nan,        nan,        nan, -1115.4952,\n",
      "               nan,        nan,        nan,        nan, -1113.1968, -1120.7939,\n",
      "        -1112.1206,        nan, -1111.8613,        nan, -1118.3883, -1113.3770,\n",
      "               nan,        nan, -1126.1505, -1123.9574,        nan,        nan,\n",
      "        -1119.5140,        nan,        nan, -1126.7969,        nan, -1115.3036,\n",
      "        -1097.1621,        nan, -1120.0615,        nan, -1128.5667,        nan,\n",
      "        -1114.6780,        nan,        nan, -1113.3950,        nan, -1117.1260,\n",
      "        -1110.7346, -1125.5327, -1122.4462,        nan,        nan, -1119.1445,\n",
      "        -1086.7136, -1113.0406,        nan,        nan, -1124.6785,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1135.2676, -1126.5723, -1118.5764,        nan,        nan,        nan,\n",
      "               nan,        nan, -1115.0371,        nan, -1129.5154, -1113.4531,\n",
      "               nan, -1118.2473, -1119.4965, -1117.1183,        nan,        nan,\n",
      "        -1115.9548,        nan, -1133.8123,        nan,        nan,        nan,\n",
      "               nan,        nan, -1115.2312,        nan,        nan,        nan,\n",
      "        -1102.9874, -1126.4338, -1086.8602, -1117.9387, -1121.2550,        nan,\n",
      "        -1120.6085,        nan, -1119.3884, -1107.5028,        nan,        nan,\n",
      "        -1122.2966,        nan, -1116.9834,        nan, -1120.1611, -1115.7190,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1113.5127,        nan, -1115.9147, -1105.3258,        nan, -1122.3256,\n",
      "        -1107.2441,        nan, -1102.9030,        nan,        nan, -1132.9218,\n",
      "        -1131.7314, -1123.6377, -1127.9863, -1115.6353, -1135.7898,        nan,\n",
      "               nan, -1118.4406,        nan,        nan,        nan, -1125.2134,\n",
      "        -1112.8760, -1125.1024, -1117.0566,        nan,        nan,        nan,\n",
      "        -1122.9790,        nan, -1130.7568, -1110.6272, -1110.4586,        nan,\n",
      "               nan, -1117.4697,        nan,        nan,        nan,        nan,\n",
      "        -1127.9102,        nan,        nan,        nan,        nan, -1128.4833,\n",
      "        -1124.5562, -1123.8575,        nan, -1129.9863,        nan,        nan,\n",
      "        -1114.6096,        nan, -1121.4727,        nan,        nan, -1130.6108,\n",
      "        -1122.7219,        nan, -1113.7996, -1119.3679, -1118.4697, -1118.8864,\n",
      "               nan, -1113.6213, -1133.0972,        nan, -1122.7844, -1113.0173,\n",
      "        -1112.0393, -1132.1135, -1122.2489,        nan, -1128.2906,        nan,\n",
      "        -1118.8472, -1133.2235, -1123.5842, -1117.8750,        nan, -1119.5516,\n",
      "        -1109.8624, -1121.4688, -1103.7797,        nan, -1117.9117, -1116.9542,\n",
      "        -1122.8348,        nan, -1110.1104, -1134.5166, -1114.0552,        nan,\n",
      "               nan,        nan, -1118.6080, -1135.4216, -1127.5333, -1109.0292,\n",
      "               nan, -1119.3472, -1121.4280, -1107.9485,        nan, -1119.6409,\n",
      "               nan, -1122.3008, -1124.6736, -1117.4529,        nan, -1128.0225,\n",
      "               nan, -1116.2463, -1121.9031,        nan,        nan, -1129.5564,\n",
      "               nan, -1111.5658, -1127.4023, -1125.0532,        nan,        nan,\n",
      "               nan, -1117.4952, -1109.2244,        nan,        nan, -1124.6310,\n",
      "        -1124.7959,        nan, -1110.8225,        nan,        nan,        nan,\n",
      "               nan, -1125.3474, -1118.5132,        nan, -1124.8674, -1121.3373,\n",
      "               nan, -1122.2983, -1114.4357, -1119.0665,        nan,        nan,\n",
      "               nan,        nan, -1119.2955, -1102.2760,        nan, -1106.5935,\n",
      "               nan,        nan,        nan, -1113.4421, -1114.0092,        nan,\n",
      "               nan, -1120.7629,        nan,        nan, -1128.3179, -1117.4397,\n",
      "               nan, -1119.8795,        nan,        nan, -1128.1765, -1124.2283,\n",
      "        -1135.5162, -1115.4188], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "tensor([-1035.8146,        nan, -1048.0609,        nan,        nan,        nan,\n",
      "        -1014.6338, -1031.2653,        nan, -1041.6279,        nan, -1008.2188,\n",
      "        -1041.3276,        nan,        nan,        nan, -1010.3074, -1038.8048,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan, -1050.1274,\n",
      "               nan, -1017.3179, -1020.3809,        nan, -1046.6124,  -997.5385,\n",
      "        -1004.6466,        nan,        nan, -1042.2943,        nan,        nan,\n",
      "               nan,        nan, -1009.0636, -1013.7729,        nan,        nan,\n",
      "        -1009.4283,        nan, -1035.5175, -1013.9202,  -999.3223,        nan,\n",
      "        -1027.5599,        nan,        nan,        nan, -1035.2236, -1029.3356,\n",
      "        -1044.7692,        nan, -1014.5038, -1034.5779,        nan, -1017.5837,\n",
      "        -1015.8883, -1005.4174, -1011.6379, -1005.8414, -1006.2631,        nan,\n",
      "               nan, -1004.8743, -1007.6124, -1040.5526, -1008.1970,        nan,\n",
      "        -1043.4866,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1019.9299,        nan, -1039.6166,  -934.0696,        nan,        nan,\n",
      "        -1011.7476, -1031.5985,        nan,        nan, -1017.2784, -1041.9835,\n",
      "               nan, -1040.4878, -1013.2013,        nan, -1009.1671, -1017.5565,\n",
      "               nan,        nan,        nan, -1004.8047,        nan, -1002.4993,\n",
      "        -1049.3899, -1038.2292, -1016.1854, -1002.9457,        nan, -1006.3756,\n",
      "        -1021.1224, -1016.9891,        nan, -1037.4841,        nan, -1030.2412,\n",
      "               nan,        nan,        nan, -1046.8889, -1037.3003,        nan,\n",
      "               nan,        nan,        nan, -1038.1606,        nan, -1013.7192,\n",
      "               nan,        nan,        nan,        nan,        nan, -1005.5978,\n",
      "        -1009.2239,        nan,        nan,        nan,        nan, -1010.4097,\n",
      "               nan,        nan, -1007.9937, -1043.4270,        nan,        nan,\n",
      "        -1007.9522,        nan, -1036.5737, -1005.4856, -1011.2832, -1035.2155,\n",
      "               nan, -1043.2347,        nan, -1048.4304, -1042.8262, -1011.4955,\n",
      "               nan,        nan, -1047.9326,        nan, -1038.6079, -1009.2133,\n",
      "        -1019.2975, -1001.1749,        nan, -1035.5210, -1035.7605,        nan,\n",
      "               nan,        nan,        nan, -1040.8569, -1011.2435, -1042.2903,\n",
      "               nan,        nan, -1038.7502,        nan, -1006.7026,        nan,\n",
      "               nan, -1042.9509,        nan,        nan, -1029.9602,        nan,\n",
      "        -1012.4772, -1035.0918,        nan,        nan,        nan,        nan,\n",
      "        -1009.6025, -1035.1548,        nan,        nan,        nan, -1011.8667,\n",
      "        -1037.3513, -1014.5730, -1035.2352,        nan, -1022.2034,        nan,\n",
      "               nan,        nan, -1012.9154,        nan,        nan,        nan,\n",
      "               nan,        nan, -1036.4034,  -990.8585,        nan, -1006.9295,\n",
      "               nan, -1038.6931,        nan, -1040.9127, -1035.4175, -1042.9088,\n",
      "               nan, -1015.2464,        nan, -1010.2272,        nan,        nan,\n",
      "               nan,        nan, -1008.1340,        nan,        nan,        nan,\n",
      "        -1007.3929,        nan, -1045.7188,        nan, -1041.3479, -1029.8228,\n",
      "        -1005.1344, -1005.9227, -1042.1405,        nan,        nan,        nan,\n",
      "        -1038.7307,        nan,        nan, -1038.7849,        nan, -1011.8251,\n",
      "        -1042.9065, -1041.4565, -1013.7135, -1035.6118, -1027.6128, -1037.7595,\n",
      "        -1008.7434, -1026.4688, -1043.9430,        nan,        nan,        nan,\n",
      "        -1008.5556,        nan, -1053.7144, -1002.3685, -1045.7983,        nan,\n",
      "               nan, -1010.7029,        nan,        nan, -1028.0442,        nan,\n",
      "        -1018.0674,        nan, -1010.7692,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan, -1006.5554,        nan, -1043.2695, -1027.6487,        nan,\n",
      "               nan,        nan,        nan, -1044.6478, -1049.9615,        nan,\n",
      "               nan, -1011.0555,        nan, -1014.7030,        nan,        nan,\n",
      "         -984.7909, -1037.6252, -1012.9480,        nan, -1033.8286, -1038.7983,\n",
      "        -1014.3002, -1019.0831, -1003.5502, -1038.8019,        nan,        nan,\n",
      "        -1011.6678,        nan, -1033.9377,        nan,        nan, -1039.6508,\n",
      "               nan, -1008.6443, -1039.3328, -1013.8478, -1005.6492, -1004.3144,\n",
      "        -1010.9712,        nan, -1024.2452,        nan, -1009.4527,        nan,\n",
      "               nan, -1043.1178,        nan, -1037.5942, -1042.1163, -1045.8163,\n",
      "               nan, -1004.1685, -1031.0105, -1037.7605,        nan,        nan,\n",
      "        -1012.2515,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan, -1003.5828,        nan, -1035.3945, -1037.5159,\n",
      "        -1014.0214, -1007.2901,        nan,        nan,        nan,        nan,\n",
      "               nan, -1046.2715, -1008.9738,        nan,        nan,        nan,\n",
      "        -1046.4282,        nan, -1021.3816,        nan, -1024.9607,        nan,\n",
      "               nan, -1012.5237, -1022.0641, -1031.4966, -1037.3655, -1007.8789,\n",
      "               nan,        nan, -1039.7078, -1034.8433,        nan, -1018.0842,\n",
      "               nan, -1030.7432, -1035.7045, -1009.1226, -1006.2911, -1008.9973,\n",
      "        -1014.8452, -1012.0599, -1057.0192, -1021.1771, -1043.8640, -1007.2845,\n",
      "        -1010.3118, -1016.7770, -1019.2324, -1055.1941,        nan,        nan,\n",
      "               nan,        nan,        nan, -1017.5004,        nan,        nan,\n",
      "        -1009.0837, -1013.4769, -1037.6661, -1011.2878,        nan, -1045.6056,\n",
      "               nan, -1037.0131,        nan,        nan,        nan,        nan,\n",
      "        -1004.3292,        nan, -1030.3491, -1024.8159,        nan,        nan,\n",
      "               nan,        nan, -1052.1763, -1012.1584,        nan, -1009.9487,\n",
      "               nan, -1007.8809,        nan, -1039.3928,        nan, -1054.7434,\n",
      "               nan, -1031.7698,        nan,        nan, -1047.3911, -1010.0010,\n",
      "        -1007.7645, -1021.6964, -1041.2980, -1033.5656, -1042.5028,        nan,\n",
      "        -1032.8136, -1018.9202, -1012.6395, -1047.9564,        nan, -1021.7428,\n",
      "        -1033.9707, -1031.3735, -1006.4186,        nan, -1055.8314,        nan,\n",
      "        -1035.5046, -1039.9373, -1042.4971,        nan, -1022.3216,        nan,\n",
      "               nan, -1035.9174, -1010.7013, -1036.6553,        nan, -1037.9626,\n",
      "               nan, -1040.5159, -1039.1713,        nan,        nan,        nan,\n",
      "        -1044.5275, -1038.0371,        nan,        nan,        nan,        nan,\n",
      "        -1044.1951, -1023.6183, -1036.0309,        nan,        nan, -1004.3566,\n",
      "        -1008.0189, -1039.1946, -1040.2148, -1007.7351, -1027.1970,        nan,\n",
      "               nan,        nan,        nan,        nan, -1014.3656, -1060.2856,\n",
      "        -1006.8143,        nan, -1004.3894,        nan,        nan,        nan,\n",
      "        -1045.9478, -1037.5042, -1009.0944,        nan,        nan, -1010.6653,\n",
      "               nan, -1040.6160,        nan, -1045.8569, -1040.2640,        nan,\n",
      "        -1032.7966, -1009.5576, -1010.6119,        nan,        nan, -1048.7311,\n",
      "               nan, -1035.8644,        nan,        nan,        nan, -1017.6331,\n",
      "        -1017.8679, -1051.5353,        nan,        nan,        nan,        nan,\n",
      "        -1005.2743, -1037.1110, -1017.3774, -1008.4744, -1012.8568, -1041.4457,\n",
      "        -1026.4526,        nan, -1010.7809,        nan, -1039.8881,        nan,\n",
      "               nan, -1010.7606, -1050.0178, -1014.0638, -1043.1165,        nan,\n",
      "               nan, -1036.5831,        nan, -1038.9182,        nan,        nan,\n",
      "               nan,        nan, -1007.7914,        nan, -1015.6409,        nan,\n",
      "               nan,        nan, -1034.7083,        nan, -1013.1946,        nan,\n",
      "               nan,        nan,        nan, -1037.8525, -1015.6919,        nan,\n",
      "               nan,        nan,        nan, -1035.8684, -1029.4062, -1007.0863,\n",
      "        -1011.5912,        nan, -1015.2274,        nan,        nan, -1042.3774,\n",
      "               nan,        nan,        nan, -1003.1599,        nan, -1047.5742,\n",
      "               nan, -1045.9336, -1009.1516, -1033.1453, -1044.4745,        nan,\n",
      "        -1004.6067, -1036.1602,        nan,        nan, -1028.9569, -1009.5842,\n",
      "               nan,        nan, -1010.6623,        nan,        nan,        nan,\n",
      "               nan,        nan, -1009.1012, -1011.4348, -1053.3552,        nan,\n",
      "               nan, -1046.4216, -1028.8151,        nan, -1034.6177,        nan,\n",
      "               nan,        nan,        nan,        nan, -1051.2400, -1046.9551,\n",
      "        -1012.7703,        nan, -1007.2618,        nan, -1004.3342,        nan,\n",
      "               nan, -1016.5203, -1039.3662, -1003.2831, -1009.1375,        nan,\n",
      "               nan, -1009.9371,        nan, -1046.9463, -1040.8406, -1036.3915,\n",
      "               nan,        nan, -1005.3070, -1039.8293,        nan, -1034.5525,\n",
      "               nan, -1040.7694,        nan, -1036.7240, -1034.3171, -1041.0372,\n",
      "        -1041.6802, -1035.1138, -1019.4346,        nan,        nan, -1052.3545,\n",
      "               nan, -1048.1379, -1018.7058, -1008.1038, -1047.5491,        nan,\n",
      "        -1035.6572, -1017.4266, -1031.6832, -1017.6445, -1042.9137, -1008.5128,\n",
      "        -1017.5995,        nan,        nan, -1034.0588,        nan,        nan,\n",
      "        -1043.2595,        nan, -1008.8125, -1010.6509,        nan, -1011.1649,\n",
      "        -1007.9286,        nan, -1014.1633, -1042.8754, -1040.7191, -1037.8264,\n",
      "               nan, -1038.3013, -1010.8846, -1041.7437, -1008.2183,        nan,\n",
      "               nan,        nan,        nan, -1037.1987,        nan,        nan,\n",
      "               nan, -1045.0540, -1007.5244,        nan,        nan, -1018.4409,\n",
      "        -1048.2573,        nan,        nan, -1045.8685,        nan,        nan,\n",
      "               nan, -1014.9221, -1041.0979,        nan,        nan, -1035.3425,\n",
      "               nan, -1014.1599,        nan,        nan,        nan, -1020.1440,\n",
      "        -1039.0255,        nan,        nan,        nan,        nan, -1008.6482,\n",
      "        -1045.4331,        nan, -1012.3246, -1040.7079, -1019.3399, -1044.6692,\n",
      "         -997.0250,        nan, -1013.7118, -1018.8451,        nan, -1013.0370,\n",
      "        -1021.1332,        nan,        nan,        nan,        nan, -1017.0406,\n",
      "               nan, -1028.6160, -1003.4418, -1002.2345, -1038.1890, -1009.8484,\n",
      "               nan,        nan,        nan,        nan, -1043.8949,        nan,\n",
      "        -1017.2272, -1048.5369,        nan, -1027.2599,        nan,        nan,\n",
      "        -1043.6235,        nan, -1039.7668, -1037.7450,        nan, -1022.2250,\n",
      "               nan,        nan], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "tensor([-1095.0120, -1085.8435,        nan,        nan,        nan,        nan,\n",
      "        -1076.6414, -1081.7272,        nan,        nan, -1084.2634,        nan,\n",
      "        -1100.8279, -1084.5005, -1058.5156,        nan, -1060.7762, -1081.6842,\n",
      "        -1057.4125, -1077.2778,        nan,        nan, -1062.1036, -1076.0182,\n",
      "        -1069.1885,        nan,        nan, -1085.8433,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1097.6863,        nan,\n",
      "               nan,        nan, -1083.4825,        nan, -1068.5186,        nan,\n",
      "        -1080.6691,        nan,        nan, -1063.1804, -1054.3608,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1069.2076,        nan, -1087.2938,        nan, -1092.5107,        nan,\n",
      "        -1088.4956,        nan,        nan, -1086.1373,        nan,        nan,\n",
      "               nan, -1063.0132, -1073.3978,        nan, -1067.0172,        nan,\n",
      "        -1070.0676,        nan,        nan, -1089.0435,        nan,        nan,\n",
      "               nan, -1080.7029, -1091.5941,        nan, -1080.1721,        nan,\n",
      "        -1051.9800, -1065.1531,        nan,  -964.0781,        nan, -1055.6768,\n",
      "        -1058.1243, -1073.4945,        nan,        nan, -1053.5736,        nan,\n",
      "               nan, -1072.6289,        nan, -1083.5801, -1068.1292, -1067.3977,\n",
      "        -1086.1738,        nan, -1086.8079, -1074.7026, -1066.5654, -1062.0930,\n",
      "        -1079.7285, -1090.4772,        nan, -1009.7886, -1086.7605,        nan,\n",
      "        -1066.5884,        nan, -1073.5217,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1086.6893,        nan, -1063.3152,\n",
      "        -1067.2343,        nan, -1089.9592,        nan, -1089.7085,        nan,\n",
      "        -1052.1006, -1094.2681,        nan,        nan,        nan, -1064.6013,\n",
      "               nan, -1071.8826,        nan, -1054.3059,        nan,        nan,\n",
      "               nan,        nan,        nan, -1096.4304,        nan, -1081.3994,\n",
      "               nan, -1087.5684, -1095.0728,        nan, -1054.7712,        nan,\n",
      "        -1061.3650, -1082.3230, -1080.4087,        nan,        nan, -1059.6387,\n",
      "               nan, -1065.2126, -1080.4858,        nan, -1088.6550,        nan,\n",
      "               nan, -1067.6772,        nan, -1091.4382, -1090.0912,        nan,\n",
      "               nan,        nan,        nan, -1093.7878,        nan,        nan,\n",
      "        -1082.7466, -1061.1255, -1091.1960, -1092.5072, -1063.0442,        nan,\n",
      "               nan, -1095.5171,        nan,        nan, -1086.7227, -1079.8588,\n",
      "        -1054.1826, -1097.9200,        nan, -1064.3037, -1084.0957,        nan,\n",
      "        -1060.2817, -1077.1477, -1076.6213,        nan,        nan, -1055.1230,\n",
      "               nan, -1068.1978, -1086.9282,        nan,        nan,        nan,\n",
      "               nan, -1085.9794, -1060.5997, -1088.1201, -1083.0945, -1063.4939,\n",
      "               nan,        nan, -1093.4188, -1061.5421,        nan, -1055.6786,\n",
      "               nan,        nan,        nan,        nan, -1087.5703,        nan,\n",
      "               nan,        nan,        nan, -1053.9551,        nan,        nan,\n",
      "        -1080.8623,        nan, -1058.8729, -1063.2847,        nan, -1086.1385,\n",
      "               nan, -1069.5936, -1090.2644, -1102.0217, -1091.3889,        nan,\n",
      "               nan, -1061.9036, -1088.0051, -1060.1792, -1071.9241, -1063.4043,\n",
      "               nan, -1085.1846, -1063.3351,        nan, -1054.9042,        nan,\n",
      "        -1080.4409,        nan, -1061.8156, -1087.6190, -1065.9993,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "        -1055.8785, -1090.8364, -1096.9856,        nan, -1082.5190, -1086.1433,\n",
      "               nan,        nan,        nan, -1101.0366, -1045.8398, -1085.6174,\n",
      "               nan,        nan,        nan, -1090.3315,        nan, -1066.5897,\n",
      "               nan, -1064.1571, -1097.0187, -1053.2793,        nan, -1066.3423,\n",
      "        -1087.8020, -1057.0500, -1060.3364,        nan,        nan,        nan,\n",
      "        -1099.8206,        nan,        nan, -1089.4231, -1086.9244, -1071.8850,\n",
      "        -1082.8473, -1078.7162, -1086.3110,        nan, -1088.1274, -1088.5986,\n",
      "         -965.9801,        nan, -1060.2349, -1060.1389, -1087.0573,        nan,\n",
      "               nan,        nan, -1057.6306,        nan, -1093.7671,        nan,\n",
      "        -1061.0209, -1091.2502, -1092.4200,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1071.3608,        nan, -1046.5261,\n",
      "        -1067.8079,        nan, -1065.9095,        nan, -1057.9911,        nan,\n",
      "               nan,        nan, -1088.7480,        nan, -1094.3160,        nan,\n",
      "        -1093.9639, -1058.5581,        nan,        nan,        nan,        nan,\n",
      "        -1059.6757, -1106.2084, -1093.5100, -1104.1013,        nan,        nan,\n",
      "               nan, -1083.7070, -1069.6516, -1086.2910, -1076.2301,        nan,\n",
      "               nan,        nan, -1061.6821,        nan, -1066.0037,        nan,\n",
      "               nan,        nan, -1056.4905,        nan,        nan,        nan,\n",
      "        -1087.4653,        nan, -1059.2217, -1070.2769,        nan,        nan,\n",
      "        -1055.3528, -1068.3013,        nan, -1091.6666, -1090.0962, -1060.2935,\n",
      "        -1058.6736,        nan, -1086.1074,        nan,        nan, -1081.5006,\n",
      "               nan,        nan, -1086.7336, -1070.9612,        nan,        nan,\n",
      "        -1076.8323, -1056.4237, -1093.8262, -1069.8179,        nan, -1051.0321,\n",
      "               nan, -1069.9956, -1063.4878,        nan, -1097.4907,        nan,\n",
      "        -1071.9806, -1097.1007,        nan,        nan,        nan,        nan,\n",
      "               nan, -1052.3867,        nan,        nan,        nan,        nan,\n",
      "        -1053.0210,        nan, -1084.6656,        nan,        nan,        nan,\n",
      "               nan,        nan, -1081.5718,        nan, -1083.9117, -1082.3989,\n",
      "        -1084.8641, -1095.4784, -1079.1427, -1054.5802,        nan, -1070.2982,\n",
      "               nan, -1053.7085, -1068.4514, -1087.3317,        nan,        nan,\n",
      "               nan, -1070.5233, -1065.8997, -1073.8901, -1088.1884,        nan,\n",
      "        -1054.4373, -1058.3992, -1088.4463, -1086.0627,        nan,        nan,\n",
      "               nan,        nan, -1068.5925,        nan, -1088.2527,        nan,\n",
      "        -1089.0480,        nan,        nan, -1083.5850, -1057.3354, -1059.4495,\n",
      "        -1087.4531, -1089.9598,        nan,        nan, -1064.6230, -1094.4423,\n",
      "              -inf, -1084.0767, -1061.0527, -1077.2377, -1094.0593, -1073.4922,\n",
      "        -1098.1846,        nan,        nan, -1062.6357,        nan,        nan,\n",
      "        -1088.5586,        nan, -1099.7040, -1099.3430,        nan, -1064.4922,\n",
      "               nan, -1073.1680,        nan, -1044.4524, -1060.4237, -1068.1221,\n",
      "        -1059.2406,        nan, -1090.6472, -1058.9319,        nan,        nan,\n",
      "        -1085.2097, -1082.9355,        nan,        nan, -1064.3634, -1076.2838,\n",
      "        -1065.0439,        nan, -1054.2747, -1055.7228, -1077.3926,        nan,\n",
      "        -1091.3956, -1083.9006, -1059.2902, -1091.9692,        nan, -1059.7695,\n",
      "        -1066.2908,        nan,        nan, -1092.1075, -1075.4817, -1062.4690,\n",
      "               nan,        nan, -1046.3528, -1087.4912, -1095.4517, -1084.2952,\n",
      "        -1036.1702, -1094.6750, -1094.9309, -1080.1814,        nan,        nan,\n",
      "               nan,        nan,        nan, -1087.0554, -1090.8088, -1080.0154,\n",
      "               nan,        nan, -1062.3389, -1058.1774,        nan, -1087.3450,\n",
      "               nan,        nan,        nan, -1083.2573,        nan,        nan,\n",
      "        -1084.8320,        nan,        nan,        nan,        nan, -1074.5592,\n",
      "        -1092.5662, -1097.9958,        nan,        nan,        nan,        nan,\n",
      "               nan, -1060.0205, -1062.0822, -1088.4296, -1063.0601,        nan,\n",
      "        -1057.7822,        nan,        nan,        nan,        nan, -1069.6906,\n",
      "        -1051.5582,        nan,        nan,        nan, -1063.9777,        nan,\n",
      "        -1091.1071,        nan, -1070.1831,        nan,        nan,        nan,\n",
      "        -1063.8572, -1091.8390,        nan,        nan,        nan, -1094.4229,\n",
      "               nan, -1085.2886,        nan,        nan, -1084.9288, -1083.6021,\n",
      "        -1088.4324, -1093.1838,        nan, -1080.9221, -1089.8185,        nan,\n",
      "        -1055.1324, -1083.7405, -1087.7490, -1087.1295,        nan,        nan,\n",
      "               nan,        nan, -1062.6025,        nan, -1081.1272, -1085.2799,\n",
      "               nan,        nan,        nan,        nan, -1080.6184,        nan,\n",
      "        -1098.4607, -1092.6849, -1086.6278,        nan, -1086.9965,        nan,\n",
      "               nan,        nan, -1091.3296, -1056.6768, -1087.5544,        nan,\n",
      "               nan,        nan,        nan, -1058.9723,        nan, -1089.4442,\n",
      "               nan,        nan,        nan,        nan, -1064.7822,        nan,\n",
      "               nan,        nan,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan, -1086.6642, -1057.4470, -1081.7217,\n",
      "        -1066.2000, -1081.8503,        nan, -1093.2954, -1084.7188, -1079.7822,\n",
      "               nan, -1065.5272, -1059.8615, -1076.9172,        nan, -1082.6831,\n",
      "        -1084.1462, -1084.8302,        nan, -1058.1938,        nan,        nan,\n",
      "               nan, -1058.8407,        nan, -1068.9927,        nan,        nan,\n",
      "        -1045.5874,        nan, -1070.0973,        nan, -1074.4778, -1060.4503,\n",
      "               nan, -1084.6904,        nan, -1074.4153,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1095.4850,        nan,\n",
      "               nan,        nan, -1074.4617, -1086.6086,        nan, -1088.4734,\n",
      "        -1061.4286, -1089.3594, -1093.9038, -1067.8180,        nan, -1058.8809,\n",
      "        -1091.0288,        nan, -1064.1873, -1086.3665,        nan,        nan,\n",
      "        -1082.6802,        nan, -1079.9265, -1095.4448,        nan, -1072.3291,\n",
      "        -1062.2344,        nan, -1089.6083, -1078.4518,        nan, -1073.0845,\n",
      "               nan,        nan,        nan,        nan, -1056.3884,        nan,\n",
      "        -1085.5911,        nan, -1072.7678,        nan,        nan, -1060.7740,\n",
      "               nan, -1063.8040, -1066.4170,        nan, -1067.1743, -1084.2848,\n",
      "               nan, -1092.3707, -1058.9534,        nan, -1084.6477, -1060.7169,\n",
      "        -1065.4977, -1093.8263, -1070.6562,        nan,        nan, -1080.2876,\n",
      "        -1083.7605,        nan, -1056.8555,        nan,        nan,        nan,\n",
      "               nan,        nan, -1073.2300,        nan,        nan, -1065.3905,\n",
      "               nan,        nan, -1094.2191, -1086.8853,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan, -1065.2753,        nan,\n",
      "        -1062.7375, -1072.0466], device='cuda:0', grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       grad_fn=<WhereBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m snpvae\u001b[38;5;241m.\u001b[39mdec(z, sex_t[:\u001b[38;5;241m800\u001b[39m], race_t[:\u001b[38;5;241m800\u001b[39m])\n\u001b[1;32m     91\u001b[0m loss_ce \u001b[38;5;241m=\u001b[39m ce(logits, xclass[:\u001b[38;5;241m800\u001b[39m])\n\u001b[0;32m---> 92\u001b[0m loss_kl \u001b[38;5;241m=\u001b[39m \u001b[43mklloss2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# loss_C, loss_mu, C, _ = latent_loss(z)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m loss_decor \u001b[38;5;241m=\u001b[39m decor_loss(z, sex_t[:\u001b[38;5;241m800\u001b[39m], race_t[:\u001b[38;5;241m800\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36mklloss2\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     66\u001b[0m sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna,nb->nab\u001b[39m\u001b[38;5;124m'\u001b[39m, z, z)\n\u001b[1;32m     67\u001b[0m tr \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mdiagonal(offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m ld \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(ld)\n\u001b[1;32m     70\u001b[0m mu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna,na->n\u001b[39m\u001b[38;5;124m'\u001b[39m, z, z)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def rmse(a, b, mean=torch.mean):\n",
    "    return mean((a-b)**2)**0.5\n",
    "\n",
    "def pretty(x):\n",
    "    return f'{round(float(x), 4)}'\n",
    "\n",
    "class SNPVAE(nn.Module):\n",
    "    def __init__(self, snpd, ld):\n",
    "        super(SNPVAE, self).__init__()\n",
    "        self.ld = ld\n",
    "        self.enc1 = nn.Linear(snpd*3, 1000).float().cuda()\n",
    "        self.enc2 = nn.Linear(1000, ld).float().cuda()\n",
    "        self.dec1 = nn.Linear(ld+2, 1000).float().cuda()\n",
    "        self.dec20 = nn.Linear(1000, snpd).float().cuda()\n",
    "        self.dec21 = nn.Linear(1000, snpd).float().cuda()\n",
    "        self.dec22 = nn.Linear(1000, snpd).float().cuda()\n",
    "\n",
    "    def enc(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        z = self.enc2(x)\n",
    "        return z\n",
    "\n",
    "    def gen(self, n):\n",
    "        return torch.randn(n, self.ld).float().cuda()#/(10**0.5)\n",
    "\n",
    "    def dec(self, z, sex, race):\n",
    "        z = torch.cat([z, sex.unsqueeze(1), race.unsqueeze(1)], dim=1)\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x0 = self.dec20(x)\n",
    "        x1 = self.dec21(x)\n",
    "        x2 = self.dec22(x)\n",
    "        return torch.stack([x0, x1, x2], dim=1)\n",
    "\n",
    "def latent_loss(z):\n",
    "    C = z.T@z\n",
    "    mu = torch.mean(z, dim=0)\n",
    "    tgt1 = torch.eye(z.shape[-1]).float().cuda()*len(z)#/10\n",
    "    tgt2 = torch.zeros(z.shape[-1]).float().cuda()\n",
    "    lossC = rmse(C, tgt1)\n",
    "    lossmu = rmse(mu, tgt2)\n",
    "    return lossC, lossmu, C, mu\n",
    "\n",
    "def decor_loss(z, sex, race):\n",
    "    sex = sex - torch.mean(sex)\n",
    "    race = race - torch.mean(race)\n",
    "    sexp = torch.einsum('n,nz->z', sex, z)\n",
    "    racep = torch.einsum('n,nz->z', race, z)\n",
    "    tgt = torch.zeros(z.shape[-1]).float().cuda()\n",
    "    loss_sex = rmse(sexp, tgt)\n",
    "    loss_race = rmse(racep, tgt)\n",
    "    return loss_sex + loss_race\n",
    "\n",
    "def klloss(z):\n",
    "    N = z.shape[1]\n",
    "    sigma = torch.std(z, dim=0)\n",
    "    mu = torch.mean(z, dim=0)\n",
    "    loss = torch.sum(sigma**2)+torch.sum(mu**2)-2*torch.sum(torch.log(sigma))\n",
    "    return loss\n",
    "\n",
    "def klloss2(z):\n",
    "    sigma = torch.einsum('na,nb->nab', z, z)\n",
    "    tr = sigma.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)\n",
    "    ld = torch.logdet(sigma)\n",
    "    print(ld)\n",
    "    mu = torch.einsum('na,na->n', z, z)\n",
    "    return torch.sum(mu+tr-ld)\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# snpvae = SNPVAE(10433, 30)\n",
    "snpvae = SNPVAE(35621, 100)\n",
    "optim = torch.optim.Adam(snpvae.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "xclass = torch.from_numpy(snps).long().cuda()\n",
    "x = torch.cat([xclass == 0, xclass == 1, xclass == 2], dim=1).float().cuda()\n",
    "sex_t = torch.from_numpy(sex).float().cuda()\n",
    "race_t = torch.from_numpy(race).float().cuda()\n",
    "\n",
    "nepochs = 2000\n",
    "pperiod = 50\n",
    "\n",
    "for e in range(nepochs):\n",
    "    optim.zero_grad()\n",
    "    z = snpvae.enc(x[:800])\n",
    "    logits = snpvae.dec(z, sex_t[:800], race_t[:800])\n",
    "    loss_ce = ce(logits, xclass[:800])\n",
    "    loss_kl = klloss2(z)\n",
    "    # loss_C, loss_mu, C, _ = latent_loss(z)\n",
    "    loss_decor = decor_loss(z, sex_t[:800], race_t[:800])\n",
    "    # Generative loss\n",
    "    sx_t = random.randint(0,1)*torch.ones(100).long().cuda()\n",
    "    rc_t = random.randint(0,1)*torch.ones(100).long().cuda()\n",
    "    z = snpvae.gen(100)\n",
    "    logits = snpvae.dec(z, sx_t, rc_t)\n",
    "    logits = torch.cat([logits[:,0], logits[:,1], logits[:,2]], dim=1).float().cuda()\n",
    "    sx_hat = logits@sex_w + sex_i\n",
    "    rc_hat = logits@race_w + race_i\n",
    "    sex_loss = ce(torch.stack([-sx_hat, sx_hat], dim=1), sx_t)\n",
    "    race_loss = ce(torch.stack([-rc_hat, rc_hat], dim=1), rc_t)\n",
    "    # (20*loss_ce+1e-4*loss_C+1e-4*loss_mu+1e-2*loss_decor+0.01*sex_loss+0.01*race_loss).backward()\n",
    "    (loss_ce+2e-4*loss_kl+2e-4*loss_decor+0.001*sex_loss+0.001*race_loss).backward()\n",
    "    optim.step()\n",
    "    if e % pperiod == 0 or e == nepochs-1:\n",
    "        # print(f'{e} {pretty(loss_ce)} {pretty(loss_C)} {pretty(loss_mu)} {pretty(loss_decor)} {pretty(sex_loss)} {pretty(race_loss)}')\n",
    "        print(f'{e} {pretty(loss_ce)} {pretty(loss_kl)} {pretty(loss_decor)} {pretty(sex_loss)} {pretty(race_loss)}')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7125f62-be0a-4072-9bea-c272ae89c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Save VAE\n",
    "\n",
    "torch.save(snpvae.state_dict(), '/home/anton/Documents/Tulane/Research/ImageNomer/data/PNC/vae_snps_big_1000_z100_cov2.torch')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de56938-b034-4e1e-ac1d-0a2df4763d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def rmse(a, b, mean=torch.mean):\n",
    "    return mean((a-b)**2)**0.5\n",
    "\n",
    "def pretty(x):\n",
    "    return f'{round(float(x), 4)}'\n",
    "\n",
    "class SNPVAE(nn.Module):\n",
    "    def __init__(self, snpd, ld):\n",
    "        super(SNPVAE, self).__init__()\n",
    "        self.ld = ld\n",
    "        self.enc1 = nn.Linear(snpd*3, 1000).float().cuda()\n",
    "        self.enc2 = nn.Linear(1000, ld).float().cuda()\n",
    "        self.dec1 = nn.Linear(ld+2, 1000).float().cuda()\n",
    "        self.dec20 = nn.Linear(1000, snpd).float().cuda()\n",
    "        self.dec21 = nn.Linear(1000, snpd).float().cuda()\n",
    "        self.dec22 = nn.Linear(1000, snpd).float().cuda()\n",
    "\n",
    "    def enc(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        z = self.enc2(x)\n",
    "        return z\n",
    "\n",
    "    def gen(self, n):\n",
    "        return torch.randn(n, self.ld).float().cuda()/(10**0.5)\n",
    "\n",
    "    def dec(self, z, sex, race):\n",
    "        z = torch.cat([z, sex.unsqueeze(1), race.unsqueeze(1)], dim=1)\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x0 = self.dec20(x)\n",
    "        x1 = self.dec21(x)\n",
    "        x2 = self.dec22(x)\n",
    "        return torch.stack([x0, x1, x2], dim=1)\n",
    "\n",
    "def latent_loss(z):\n",
    "    C = z.T@z\n",
    "    mu = torch.mean(z, dim=0)\n",
    "    tgt1 = torch.eye(z.shape[-1]).float().cuda()*len(z)/10\n",
    "    tgt2 = torch.zeros(z.shape[-1]).float().cuda()\n",
    "    lossC = rmse(C, tgt1)\n",
    "    lossmu = rmse(mu, tgt2)\n",
    "    return lossC, lossmu, C, mu\n",
    "\n",
    "def decor_loss(z, sex, race):\n",
    "    sex = sex - torch.mean(sex)\n",
    "    race = race - torch.mean(race)\n",
    "    sexp = torch.einsum('n,nz->z', sex, z)\n",
    "    racep = torch.einsum('n,nz->z', race, z)\n",
    "    tgt = torch.zeros(z.shape[-1]).float().cuda()\n",
    "    loss_sex = rmse(sexp, tgt)\n",
    "    loss_race = rmse(racep, tgt)\n",
    "    return loss_sex + loss_race\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# snpvae = SNPVAE(10433, 30)\n",
    "snpvae = SNPVAE(35621, 100)\n",
    "snpvae.load_state_dict(torch.load('/home/anton/Documents/Tulane/Research/ImageNomer/data/PNC/vae_snps_big_1000_z100_cov2.torch'))\n",
    "snpvae.eval()\n",
    "\n",
    "xclass = torch.from_numpy(snps).long().cuda()\n",
    "x = torch.cat([xclass == 0, xclass == 1, xclass == 2], dim=1).float().cuda()\n",
    "sex_t = torch.from_numpy(sex).float().cuda()\n",
    "race_t = torch.from_numpy(race).float().cuda()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2026ff77-a122-4404-8251-f2f7cdc76deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33629, device='cuda:0')\n",
      "tensor(29759, device='cuda:0')\n",
      "tensor(35621, device='cuda:0')\n",
      "tensor(33496, device='cuda:0')\n",
      "tensor(35327, device='cuda:0')\n",
      "tensor(35504, device='cuda:0')\n",
      "tensor(35431, device='cuda:0')\n",
      "tensor(34295, device='cuda:0')\n",
      "tensor(33910, device='cuda:0')\n",
      "tensor(35308, device='cuda:0')\n",
      "tensor(29521, device='cuda:0')\n",
      "tensor(35581, device='cuda:0')\n",
      "tensor(35425, device='cuda:0')\n",
      "tensor(34509, device='cuda:0')\n",
      "tensor(32095, device='cuda:0')\n",
      "tensor(32433, device='cuda:0')\n",
      "tensor(34485, device='cuda:0')\n",
      "tensor(35492, device='cuda:0')\n",
      "tensor(35406, device='cuda:0')\n",
      "tensor(35494, device='cuda:0')\n",
      "tensor(35621, device='cuda:0')\n",
      "tensor(35478, device='cuda:0')\n",
      "tensor(33117, device='cuda:0')\n",
      "tensor(35364, device='cuda:0')\n",
      "tensor(32256, device='cuda:0')\n",
      "tensor(31379, device='cuda:0')\n",
      "tensor(31255, device='cuda:0')\n",
      "tensor(32366, device='cuda:0')\n",
      "tensor(30948, device='cuda:0')\n",
      "tensor(31583, device='cuda:0')\n",
      "tensor(34977, device='cuda:0')\n",
      "tensor(35475, device='cuda:0')\n",
      "tensor(35190, device='cuda:0')\n",
      "tensor(34390, device='cuda:0')\n",
      "tensor(34436, device='cuda:0')\n",
      "tensor(35076, device='cuda:0')\n",
      "tensor(31155, device='cuda:0')\n",
      "tensor(33870, device='cuda:0')\n",
      "tensor(34057, device='cuda:0')\n",
      "tensor(32401, device='cuda:0')\n",
      "tensor(32534, device='cuda:0')\n",
      "tensor(29066, device='cuda:0')\n",
      "tensor(35431, device='cuda:0')\n",
      "tensor(35569, device='cuda:0')\n",
      "tensor(33620, device='cuda:0')\n",
      "tensor(34561, device='cuda:0')\n",
      "tensor(32671, device='cuda:0')\n",
      "tensor(33350, device='cuda:0')\n",
      "tensor(34446, device='cuda:0')\n",
      "tensor(32176, device='cuda:0')\n",
      "tensor(34636, device='cuda:0')\n",
      "tensor(33931, device='cuda:0')\n",
      "tensor(35235, device='cuda:0')\n",
      "tensor(35395, device='cuda:0')\n",
      "tensor(34280, device='cuda:0')\n",
      "tensor(35597, device='cuda:0')\n",
      "tensor(35410, device='cuda:0')\n",
      "tensor(35104, device='cuda:0')\n",
      "tensor(33475, device='cuda:0')\n",
      "tensor(32098, device='cuda:0')\n",
      "tensor(32749, device='cuda:0')\n",
      "tensor(32597, device='cuda:0')\n",
      "tensor(35104, device='cuda:0')\n",
      "tensor(31109, device='cuda:0')\n",
      "tensor(35460, device='cuda:0')\n",
      "tensor(34875, device='cuda:0')\n",
      "tensor(35533, device='cuda:0')\n",
      "tensor(35567, device='cuda:0')\n",
      "tensor(32625, device='cuda:0')\n",
      "tensor(35535, device='cuda:0')\n",
      "tensor(34883, device='cuda:0')\n",
      "tensor(32039, device='cuda:0')\n",
      "tensor(35366, device='cuda:0')\n",
      "tensor(33428, device='cuda:0')\n",
      "tensor(35577, device='cuda:0')\n",
      "tensor(34618, device='cuda:0')\n",
      "tensor(33373, device='cuda:0')\n",
      "tensor(35582, device='cuda:0')\n",
      "tensor(31245, device='cuda:0')\n",
      "tensor(34918, device='cuda:0')\n",
      "tensor(32785, device='cuda:0')\n",
      "tensor(34320, device='cuda:0')\n",
      "tensor(33529, device='cuda:0')\n",
      "tensor(34103, device='cuda:0')\n",
      "tensor(30486, device='cuda:0')\n",
      "tensor(35516, device='cuda:0')\n",
      "tensor(35604, device='cuda:0')\n",
      "tensor(35410, device='cuda:0')\n",
      "tensor(35532, device='cuda:0')\n",
      "tensor(35529, device='cuda:0')\n",
      "tensor(31999, device='cuda:0')\n",
      "tensor(32035, device='cuda:0')\n",
      "tensor(35412, device='cuda:0')\n",
      "tensor(33646, device='cuda:0')\n",
      "tensor(31700, device='cuda:0')\n",
      "tensor(33250, device='cuda:0')\n",
      "tensor(35599, device='cuda:0')\n",
      "tensor(35563, device='cuda:0')\n",
      "tensor(35384, device='cuda:0')\n",
      "tensor(32986, device='cuda:0')\n",
      "tensor(32038, device='cuda:0')\n",
      "tensor(32487, device='cuda:0')\n",
      "tensor(34416, device='cuda:0')\n",
      "tensor(30879, device='cuda:0')\n",
      "tensor(35601, device='cuda:0')\n",
      "tensor(30441, device='cuda:0')\n",
      "tensor(34251, device='cuda:0')\n",
      "tensor(35621, device='cuda:0')\n",
      "tensor(33781, device='cuda:0')\n",
      "tensor(34888, device='cuda:0')\n",
      "tensor(34252, device='cuda:0')\n",
      "tensor(35532, device='cuda:0')\n",
      "tensor(34447, device='cuda:0')\n",
      "tensor(35585, device='cuda:0')\n",
      "tensor(35372, device='cuda:0')\n",
      "tensor(33200, device='cuda:0')\n",
      "tensor(30475, device='cuda:0')\n",
      "tensor(35621, device='cuda:0')\n",
      "tensor(35621, device='cuda:0')\n",
      "tensor(29704, device='cuda:0')\n",
      "tensor(35356, device='cuda:0')\n",
      "tensor(34462, device='cuda:0')\n",
      "tensor(33511, device='cuda:0')\n",
      "tensor(32400, device='cuda:0')\n",
      "tensor(32202, device='cuda:0')\n",
      "tensor(35453, device='cuda:0')\n",
      "tensor(31632, device='cuda:0')\n",
      "tensor(29875, device='cuda:0')\n",
      "tensor(34308, device='cuda:0')\n",
      "tensor(35260, device='cuda:0')\n",
      "tensor(32905, device='cuda:0')\n",
      "tensor(31616, device='cuda:0')\n",
      "tensor(34473, device='cuda:0')\n",
      "tensor(33568, device='cuda:0')\n",
      "tensor(28750, device='cuda:0')\n",
      "tensor(33637, device='cuda:0')\n",
      "tensor(35392, device='cuda:0')\n",
      "tensor(35612, device='cuda:0')\n",
      "tensor(35450, device='cuda:0')\n",
      "tensor(35553, device='cuda:0')\n",
      "tensor(31968, device='cuda:0')\n",
      "tensor(32390, device='cuda:0')\n",
      "tensor(33638, device='cuda:0')\n",
      "tensor(35528, device='cuda:0')\n",
      "tensor(34716, device='cuda:0')\n",
      "tensor(35611, device='cuda:0')\n",
      "tensor(35068, device='cuda:0')\n",
      "tensor(35470, device='cuda:0')\n",
      "tensor(35533, device='cuda:0')\n",
      "tensor(35481, device='cuda:0')\n",
      "tensor(33011, device='cuda:0')\n",
      "tensor(35555, device='cuda:0')\n",
      "tensor(35449, device='cuda:0')\n",
      "tensor(29399, device='cuda:0')\n",
      "tensor(33565, device='cuda:0')\n",
      "tensor(34019, device='cuda:0')\n",
      "tensor(34681, device='cuda:0')\n",
      "tensor(31892, device='cuda:0')\n",
      "tensor(28831, device='cuda:0')\n",
      "tensor(35558, device='cuda:0')\n",
      "tensor(34419, device='cuda:0')\n",
      "tensor(34632, device='cuda:0')\n",
      "tensor(32970, device='cuda:0')\n",
      "tensor(35452, device='cuda:0')\n",
      "tensor(33824, device='cuda:0')\n",
      "tensor(27572, device='cuda:0')\n",
      "tensor(35315, device='cuda:0')\n",
      "tensor(29640, device='cuda:0')\n",
      "tensor(35445, device='cuda:0')\n",
      "tensor(35444, device='cuda:0')\n",
      "tensor(33572, device='cuda:0')\n",
      "tensor(29070, device='cuda:0')\n",
      "tensor(32849, device='cuda:0')\n",
      "tensor(35267, device='cuda:0')\n",
      "tensor(34544, device='cuda:0')\n",
      "tensor(34952, device='cuda:0')\n",
      "tensor(35513, device='cuda:0')\n",
      "tensor(33640, device='cuda:0')\n",
      "tensor(34486, device='cuda:0')\n",
      "tensor(35584, device='cuda:0')\n",
      "tensor(35405, device='cuda:0')\n",
      "tensor(34342, device='cuda:0')\n",
      "tensor(31827, device='cuda:0')\n",
      "tensor(35589, device='cuda:0')\n",
      "tensor(35375, device='cuda:0')\n",
      "tensor(31349, device='cuda:0')\n",
      "tensor(33854, device='cuda:0')\n",
      "tensor(34254, device='cuda:0')\n",
      "tensor(35616, device='cuda:0')\n",
      "tensor(34860, device='cuda:0')\n",
      "tensor(31028, device='cuda:0')\n",
      "tensor(31323, device='cuda:0')\n",
      "tensor(33636, device='cuda:0')\n",
      "tensor(30513, device='cuda:0')\n",
      "tensor(28662, device='cuda:0')\n",
      "tensor(35058, device='cuda:0')\n",
      "tensor(35568, device='cuda:0')\n",
      "tensor(34428, device='cuda:0')\n",
      "tensor(35146, device='cuda:0')\n",
      "tensor(31049, device='cuda:0')\n",
      "tensor([2, 2, 1,  ..., 2, 2, 2], device='cuda:0')\n",
      "tensor([2, 2, 1,  ..., 2, 2, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = snpvae.enc(x)\n",
    "    logits = snpvae.dec(z, sex_t, race_t)\n",
    "    xhat = torch.argmax(logits, dim=1)\n",
    "\n",
    "for i in range(800,1000):\n",
    "    print(torch.sum(xhat[i] == xclass[i]))\n",
    "print(xhat[2])\n",
    "print(xclass[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774b0d69-6e50-4a69-aee2-c2706a4c8a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, fcd, ld):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fcd = fcd\n",
    "        self.ld = ld\n",
    "        self.enc1 = nn.Linear(fcd, 1000).float().cuda()\n",
    "        self.enc2 = nn.Linear(1000, ld).float().cuda()\n",
    "        self.dec1 = nn.Linear(ld+6, 1000).float().cuda()\n",
    "        self.dec2 = nn.Linear(1000, fcd).float().cuda()\n",
    "\n",
    "    def enc(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        z = self.enc2(x)\n",
    "        return z\n",
    "\n",
    "    def gen(self, n):\n",
    "        return torch.randn(n, self.ld).float().cuda()/(10**0.5)\n",
    "    \n",
    "    def dec(self, z, age, sex, race, rest, nback, emoid):\n",
    "        z = torch.cat([z, age.unsqueeze(1), sex.unsqueeze(1), race.unsqueeze(1), \n",
    "                       rest.unsqueeze(1), nback.unsqueeze(1), emoid.unsqueeze(1)], dim=1)\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = self.dec2(x)\n",
    "        return x\n",
    "\n",
    "def rmse(a, b, mean=torch.mean):\n",
    "    return mean((a-b)**2)**0.5\n",
    "\n",
    "def pretty(x):\n",
    "    return f'{round(float(x), 4)}'\n",
    "\n",
    "vae = VAE(34716, 30)\n",
    "vae.load_state_dict(torch.load('/home/anton/Documents/Tulane/Research/ImageNomer/data/PNC/vae_1000_z30_cov6.torch'))\n",
    "vae.eval()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebb6ce0-6629-42ec-8809-a7fd986bd873",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (40000x100 and 50x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     ztask \u001b[38;5;241m=\u001b[39m zemoid\n\u001b[0;32m---> 54\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mztask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midcs1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzsnp\u001b[49m\u001b[43m[\u001b[49m\u001b[43midcs2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss_cl \u001b[38;5;241m=\u001b[39m cl_loss(sims, pidcs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m loss_cl\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mSimilarity.forward\u001b[0;34m(self, zfc, zsnp)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, zfc, zsnp):\n\u001b[1;32m      9\u001b[0m     zfc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfca1(zfc))\n\u001b[0;32m---> 10\u001b[0m     zsnp \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfcb1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzsnp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([zfc, zsnp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(z)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (40000x100 and 50x20)"
     ]
    }
   ],
   "source": [
    "class Similarity(nn.Module):\n",
    "    def __init__(self, ldfc, ldsnp):\n",
    "        super(Similarity, self).__init__()\n",
    "        self.fca1 = nn.Linear(ldfc, 20).float().cuda()\n",
    "        self.fcb1 = nn.Linear(ldsnp, 20).float().cuda()\n",
    "        self.fc2 = nn.Linear(40, 1).float().cuda()\n",
    "\n",
    "    def forward(self, zfc, zsnp):\n",
    "        zfc = F.relu(self.fca1(zfc))\n",
    "        zsnp = F.relu(self.fcb1(zsnp))\n",
    "        z = torch.cat([zfc, zsnp], dim=1)\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "def cl_loss(sims, pidcs, tau):\n",
    "    pp = sims[pidcs]\n",
    "    ep = torch.sum(torch.exp(pp/tau))\n",
    "    en = torch.sum(torch.exp(sims/tau))\n",
    "    loss = -torch.log(ep/en)\n",
    "    return loss\n",
    "\n",
    "sim = Similarity(30, 50)\n",
    "optim = torch.optim.Adam(sim.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "with torch.no_grad():\n",
    "    zrest = vae.enc(torch.from_numpy(rest).float().cuda())\n",
    "    znback = vae.enc(torch.from_numpy(nback).float().cuda())\n",
    "    zemoid = vae.enc(torch.from_numpy(emoid).float().cuda())\n",
    "    zsnp = snpvae.enc(x)\n",
    "\n",
    "nepochs = 20000\n",
    "pperiod = 500\n",
    "nb = 200\n",
    "\n",
    "for e in range(nepochs):\n",
    "    optim.zero_grad()\n",
    "    sidcs = np.random.permutation(len(x))[:nb]\n",
    "    idcs1 = []\n",
    "    idcs2 = []\n",
    "    pidcs = []\n",
    "    for i in range(nb):\n",
    "        idcs1.append(np.ones(nb)*sidcs[i])\n",
    "        idcs2.append(sidcs)\n",
    "        pidcs.append(nb*i+i)\n",
    "    idcs1 = np.array(np.concatenate(idcs1))\n",
    "    idcs2 = np.array(np.concatenate(idcs2))\n",
    "    pidcs = np.array(pidcs)\n",
    "    if e % 3 == 0:\n",
    "        ztask = zrest\n",
    "    elif e % 3 == 1:\n",
    "        ztask = znback\n",
    "    else:\n",
    "        ztask = zemoid\n",
    "    sims = sim(ztask[idcs1], zsnp[idcs2])\n",
    "    loss_cl = cl_loss(sims, pidcs, 1)\n",
    "    loss_cl.backward()\n",
    "    optim.step()\n",
    "    if e % pperiod == 0 or e == nepochs-1:\n",
    "        print(f'{e} {pretty(loss_cl)}')\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf2b5d07-7c7b-4ffa-9ac6-02241bbfad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5060987525150905\n",
      "1 0.5055681287726358\n",
      "1 0.5063547686116701\n",
      "1 0.506019879275654\n",
      "1 0.5075904225352114\n",
      "1 0.5070614084507042\n",
      "1 0.5079449496981892\n",
      "1 0.5068706639839033\n",
      "1 0.506313722334004\n",
      "1 0.5060567404426559\n",
      "1 0.5034149698189134\n",
      "1 0.5059128370221327\n",
      "1 0.5074739637826963\n",
      "1 0.5060327565392354\n",
      "1 0.5091603219315896\n",
      "1 0.5066175452716298\n",
      "1 0.5060531187122737\n",
      "1 0.5069287726358149\n",
      "1 0.5048663179074446\n",
      "1 0.505982615694165\n",
      "---\n",
      "1.0\n",
      "0.0\n",
      "0.5064161327967807\n",
      "0.0011519840217475612\n"
     ]
    }
   ],
   "source": [
    "# Test PC imputation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "xreal = x.detach().cpu().numpy()\n",
    "\n",
    "xtr = xreal[:800]\n",
    "xt = xreal[800:]\n",
    "sx = torch.from_numpy(sex[800:]).float().cuda()\n",
    "rc = torch.from_numpy(race[800:]).float().cuda()\n",
    "x2t = snps[800:]\n",
    "\n",
    "accs_pca = []\n",
    "accs_vae = []\n",
    "# accs_sample = []\n",
    "n = 35000\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    # xtr, xt, x2tr, x2t, _, zr, _, zn, _, ze, _, sx, _, rc = train_test_split(xreal, snps, zrest, znback, zemoid, sex_t, race_t, train_size=0.8)\n",
    "\n",
    "    # sx[:] = torch.randint(low=0,high=2,size=[len(sx)])\n",
    "    # rc[:] = torch.randint(low=0,high=2,size=[len(rc)])\n",
    "    \n",
    "    miss = np.random.permutation(snps.shape[-1])[:n]\n",
    "    # pca = PCA(n_components=1).fit(xtr)\n",
    "    \n",
    "    xtt = xt+0\n",
    "    cls = np.random.randint(low=0, high=3, size=(n))\n",
    "    xtt[:,miss] = (cls == 0)+0\n",
    "    xtt[:,miss+1*snps.shape[-1]] = (cls == 1)+0\n",
    "    xtt[:,miss+2*snps.shape[-1]] = (cls == 2)+0\n",
    "    \n",
    "    # xtt_pca = pca.transform(xtt)\n",
    "    # xtt_bak = pca.inverse_transform(xtt_pca)\n",
    "    \n",
    "    # xtt_bak = xtt_bak.reshape(xtt_bak.shape[0], 3, snps.shape[-1])\n",
    "    # xtt_bak = np.argmax(xtt_bak, axis=1)\n",
    "    \n",
    "    # acc = np.mean(np.sum(xtt_bak[:,miss] == x2t[:,miss], axis=1))\n",
    "    # accs_pca.append(acc/n)\n",
    "    accs_pca.append(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = snpvae.enc(torch.from_numpy(xtt).float().cuda())\n",
    "        logits = snpvae.dec(z, sx, rc)\n",
    "        logits = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    acc = np.mean(np.sum(logits[:,miss] == x2t[:,miss], axis=1))\n",
    "    accs_vae.append(acc/n)\n",
    "\n",
    "    print(accs_pca[-1], accs_vae[-1])\n",
    "\n",
    "    # myaccs = []\n",
    "    \n",
    "    # for i in range(len(zr)):\n",
    "    #     with torch.no_grad():\n",
    "    #         zz = snpvae.gen(100)\n",
    "    #         sims1 = sim(zr[i:i+1].repeat([100,1]), zz)\n",
    "    #         sims2 = sim(zn[i:i+1].repeat([100,1]), zz)\n",
    "    #         sims3 = sim(ze[i:i+1].repeat([100,1]), zz)\n",
    "    #         sims = sims1+sims2+sims3\n",
    "\n",
    "    #         idcs = torch.argsort(sims, descending=True)\n",
    "    #         best = 0\n",
    "    #         for j in idcs[:50]:\n",
    "    #             logits = snpvae.dec(zz[j:j+1], sx[i:i+1], rc[i:i+1])\n",
    "    #             logits = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "    #             acc = np.sum(logits[0,miss] == x2t[i,miss])\n",
    "    #             if acc > best:\n",
    "    #                 best = acc\n",
    "    #         myaccs.append(best/n)\n",
    "\n",
    "    # print(np.mean(myaccs))\n",
    "\n",
    "print('---')\n",
    "print(np.mean(accs_pca))\n",
    "print(np.std(accs_pca))\n",
    "print(np.mean(accs_vae))\n",
    "print(np.std(accs_vae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ad0eb-cd91-4849-b514-bbeb98199716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
